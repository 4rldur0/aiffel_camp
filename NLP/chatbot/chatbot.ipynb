{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?\n",
    "```python\n",
    "def build_corpus(df, length, labels):\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "    tokenizer = Mecab(dicpath=r\"C:\\mecab\\share\\mecab-ko-dic\")\n",
    "    df_copy = df.copy()\n",
    "    # 각 레이블에 전처리, 토큰화, 길이제한 적용\n",
    "    for l in labels:\n",
    "        df_copy[l] = df_copy[l].apply(lambda x: preprocessing(x))\n",
    "        df_copy[l] = df_copy[l].apply(lambda x: tokenizer.morphs(x))\n",
    "        df_copy = df_copy[df_copy[l].apply(len) <= length]\n",
    "    removed_num = df.shape[0] - df_copy.shape[0]\n",
    "    print(\n",
    "        f\"데이터 길이 제한으로 사라진 데이터 수 : {removed_num}, {(removed_num)/df.shape[0]*100}%\"\n",
    "    )\n",
    "    # 중복 제거\n",
    "    df_copy = df_copy.drop_duplicates(labels[0])\n",
    "    df_copy = df_copy.drop_duplicates(labels[1])\n",
    "\n",
    "    print(\n",
    "        f\"데이터 중복으로 사라진 데이터 수 : {df.shape[0]-df_copy.shape[0]-removed_num}, {(df.shape[0]-df_copy.shape[0]-removed_num)/df.shape[0]*100}%\"\n",
    "    )\n",
    "    return df_copy, tokenizer\n",
    "```\n",
    "2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?\n",
    "```text\n",
    "...\n",
    "poch  9: 100%|██████████| 213/213 [00:05<00:00, 40.80it/s, Loss 0.7520]\n",
    "Valid Loss 3.0349\n",
    "Epoch 10: 100%|██████████| 213/213 [00:05<00:00, 40.81it/s, Loss 0.6610]\n",
    "Valid Loss 2.9380\n",
    "```\n",
    "\n",
    "3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?\n",
    "```text\n",
    "Input: 지루하다, 놀러가고 싶어.\n",
    "Predicted translation: 좋 아 하 는 걸 만나 봐요 . <end>\n",
    "\n",
    "Input: 오늘 일찍 일어났더니 피곤하다.\n",
    "Predicted translation: 그렇 기 도 해요 . <end>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 \n",
    "\n",
    "https://github.com/songys/Chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ChatbotData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"label\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리, 토큰화, 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>화이트데이에 고백할까요?</td>\n",
       "      <td>선물을 주면서 솔직하고 당당하게 고백해보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?</td>\n",
       "      <td>그 사람을 위해서는 그러면 안돼요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>회식하는데 나만 챙겨줘. 썸임?</td>\n",
       "      <td>호감이 있을 수도 있어요. 그렇지만 조금 더 상황을 지켜보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Q                                    A\n",
       "3                       3박4일 정도 놀러가고 싶다                          여행은 언제나 좋죠.\n",
       "6                               SD카드 안돼                   다시 새로 사는 게 마음 편해요.\n",
       "9                     SNS 시간낭비인데 자꾸 보게됨                        시간을 정하고 해보세요.\n",
       "12                          가끔 뭐하는지 궁금해                        그 사람도 그럴 거예요.\n",
       "18                       가스불 켜놓고 나온거 같아                  빨리 집에 돌아가서 끄고 나오세요.\n",
       "...                                 ...                                  ...\n",
       "11807                     화이트데이에 고백할까요?            선물을 주면서 솔직하고 당당하게 고백해보세요.\n",
       "11809  확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?                  그 사람을 위해서는 그러면 안돼요.\n",
       "11816                 회식하는데 나만 챙겨줘. 썸임?  호감이 있을 수도 있어요. 그렇지만 조금 더 상황을 지켜보세요.\n",
       "11819                    훔쳐보는 것도 눈치 보임.                        훔쳐보는 거 티나나봐요.\n",
       "11822                        힘들어서 결혼할까봐                   도피성 결혼은 하지 않길 바라요.\n",
       "\n",
       "[4111 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복 질문 확인\n",
    "df[(df.duplicated(\"Q\") == True) | (df.duplicated(\"A\") == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>고양이 키우고 싶어</td>\n",
       "      <td>자신을 먼저 키우세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>고양이 키우고 싶어</td>\n",
       "      <td>가족들과 상의해보세요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q             A\n",
       "195  고양이 키우고 싶어  자신을 먼저 키우세요.\n",
       "196  고양이 키우고 싶어  가족들과 상의해보세요."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Q\"] == \"고양이 키우고 싶어\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    import re\n",
    "\n",
    "    sentence = sentence.lower()  # 소문자화\n",
    "    sentence = re.sub(r\"[^가-힣1-9a-zA-Z?.!,]+\", \" \", sentence)  # 기타 문자 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(df, length, labels):\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "    tokenizer = Mecab(dicpath=r\"C:\\mecab\\share\\mecab-ko-dic\")\n",
    "    df_copy = df.copy()\n",
    "    # 각 레이블에 전처리, 토큰화, 길이제한 적용\n",
    "    for l in labels:\n",
    "        df_copy[l] = df_copy[l].apply(lambda x: preprocessing(x))\n",
    "        df_copy[l] = df_copy[l].apply(lambda x: tokenizer.morphs(x))\n",
    "        df_copy = df_copy[df_copy[l].apply(len) <= length]\n",
    "    removed_num = df.shape[0] - df_copy.shape[0]\n",
    "    print(\n",
    "        f\"데이터 길이 제한으로 사라진 데이터 수 : {removed_num}, {(removed_num)/df.shape[0]*100}%\"\n",
    "    )\n",
    "    # 중복 제거\n",
    "    df_copy = df_copy.drop_duplicates(labels[0])\n",
    "    df_copy = df_copy.drop_duplicates(labels[1])\n",
    "\n",
    "    print(\n",
    "        f\"데이터 중복으로 사라진 데이터 수 : {df.shape[0]-df_copy.shape[0]-removed_num}, {(df.shape[0]-df_copy.shape[0]-removed_num)/df.shape[0]*100}%\"\n",
    "    )\n",
    "    return df_copy, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 길이 제한으로 사라진 데이터 수 : 151, 1.277171614649412%\n",
      "데이터 중복으로 사라진 데이터 수 : 4110, 34.762750570921085%\n"
     ]
    }
   ],
   "source": [
    "df_clean, mecab = build_corpus(df, 20, df.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7562, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 증강\n",
    "\n",
    "사전 훈련된 wor2vec을 이용한 Lexical Substitution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load(\"data/word2vec_ko.model\")\n",
    "wv = word_vectors.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non 오류 찾기\n",
    "def find_nan(sample_sentence, wv):\n",
    "\n",
    "    # . , ? ! 등 기호가 대체되지 않게 단어만 선택\n",
    "    valid_tokens = [tok for tok in sample_sentence if tok.isalnum()]\n",
    "\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    result = []\n",
    "    for tok in sample_sentence:\n",
    "        if tok is selected_tok:\n",
    "            try:\n",
    "                print(tok, wv.most_similar(tok)[0][0])\n",
    "            except:\n",
    "                print(tok, \"사전에 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('假裝', 0.6139413118362427),\n",
       " ('칠검', 0.6112638115882874),\n",
       " ('빛난', 0.6083515286445618),\n",
       " ('큰데', 0.603387176990509),\n",
       " ('돋보인', 0.6031648516654968),\n",
       " ('가까워서', 0.6004668474197388),\n",
       " ('심했었', 0.5907561182975769),\n",
       " ('적다', 0.5850600004196167),\n",
       " ('家長', 0.5813945531845093),\n",
       " ('슬퍼했으며', 0.5703328251838684)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"비싼데\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean[\"Q\"][:100].apply(lambda x: find_nan(x, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sample_sentence, wv):\n",
    "\n",
    "    # . , ? ! 등 기호가 대체되지 않게 단어만 선택\n",
    "    valid_tokens = [tok for tok in sample_sentence if tok.isalnum()]\n",
    "\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    result = []\n",
    "    for tok in sample_sentence:\n",
    "        if tok is selected_tok:\n",
    "            try:\n",
    "                result.append(wv.most_similar(tok)[0][0])\n",
    "            except:\n",
    "                result.append(tok)\n",
    "        else:\n",
    "            result.append(tok)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 \n",
    "df_aug = pd.DataFrame()\n",
    "df_aug[\"Q\"] = df_clean[\"Q\"].apply(lambda x: lexical_sub(x, wv))\n",
    "df_aug[\"A\"] = df_clean[\"A\"].apply(lambda x: lexical_sub(x, wv))\n",
    "\n",
    "df_combined = pd.concat([df_clean, df_aug], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['12', '시경', '땡', '!'], ['12', '시', '땡', '!'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 증강된 데이터 비교 \n",
    "df_aug[\"Q\"][0], df_clean[\"Q\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15124, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2만개 가량으로 증강\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"A\"] = df_combined[\"A\"].apply(lambda x: [\"<start>\"] + x + [\"<end>\"])\n",
    "# df_clean[\"A\"] = df_clean[\"A\"].apply(lambda x: [\"<start>\"] + x + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, NUM_WORDS):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, filters=\"\")\n",
    "    corpus = []\n",
    "    for _, sentence in df.iterrows():\n",
    "        corpus.extend(sentence[df.columns[0]])\n",
    "        corpus.extend(sentence[df.columns[1]])\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    temp_df = df.copy()\n",
    "    temp_df[\"Q\"] = tokenizer.texts_to_sequences(df[\"Q\"])\n",
    "    temp_df[\"A\"] = tokenizer.texts_to_sequences(df[\"A\"])\n",
    "    max_len = max(temp_df[\"Q\"].apply(len).max(), temp_df[\"A\"].apply(len).max())\n",
    "\n",
    "    Q = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        temp_df[\"Q\"], maxlen=max_len, padding=\"post\"\n",
    "    )\n",
    "    A = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        temp_df[\"A\"], maxlen=max_len, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    return Q, A, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7545"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc, dec, tokenizer = tokenize(df_combined, None)\n",
    "max_len = enc.shape[1]\n",
    "# 총 단어 개수\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6190"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 증강하지 않은 데이터\n",
    "\n",
    "# enc_clean, dec_clean, tokenizer_clean = tokenize(df_clean, None)\n",
    "# max_len_clean = enc_clean.shape[1]\n",
    "# # 총 단어 개수\n",
    "# len(tokenizer_clean.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할\n",
    "enc_train, enc_test, dec_train, dec_test = train_test_split(\n",
    "    enc, dec, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13611, 22), (1513, 22), (13611, 22), (1513, 22))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, enc_test.shape, dec_train.shape, dec_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세트로 변형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = tf.data.Dataset.from_tensor_slices((enc_test, dec_test)).batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강하지 않은 데이터\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# dataset_clean = tf.data.Dataset.from_tensor_slices(\n",
    "#     (\n",
    "#         enc_clean,\n",
    "#         dec_clean,\n",
    "#     )\n",
    "# ).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델\n",
    "\n",
    "[트랜스포머 모델](../../model/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"chatbot.ipynb\"))\n",
    "model_dir = os.path.abspath(os.path.join(current_dir, \"../../model\"))\n",
    "sys.path.append(model_dir)\n",
    "\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformer' from 'c:\\\\Users\\\\zzoon\\\\projects\\\\aiffel_camp\\\\model\\\\transformer.py'>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 외부 모듈 수정하면 다시 불러오는 거\n",
    "import importlib\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words가 None인 무제한인 경우 단어 사전의 길이로 사전의 크기를 정의\n",
    "if tokenizer.num_words:\n",
    "\n",
    "\n",
    "    VOCAB_SZIE = tokenizer.num_words\n",
    "\n",
    "else:\n",
    "    VOCAB_SZIE = len(tokenizer.word_index)\n",
    "\n",
    "\n",
    "\n",
    "model = transformer.Transformer(\n",
    "    n_layers=2,\n",
    "\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    d_ff=512,\n",
    "\n",
    "    src_vocab_size=VOCAB_SZIE,\n",
    "    tgt_vocab_size=VOCAB_SZIE,\n",
    "    pos_len=max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 213/213 [00:12<00:00, 17.19it/s, Loss 3.3729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.6366\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 은 사람 이 있 는 것 도 좋 을 거 예요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 잘 하 고 있 는 게 좋 은 거 예요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 좋 은 사람 이 있 었 나 봐요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 많이 하 고 싶 은 아니 에요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 213/213 [00:05<00:00, 39.22it/s, Loss 3.2017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.5129\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 것 도 좋 아요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 잘 하 고 있 나 봐요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 좋 아 하 지 마세요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 많이 하 고 싶 네요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 213/213 [00:05<00:00, 41.09it/s, Loss 2.8826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.3488\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 것 도 좋 아요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 지금 은 항상 괜찮 아요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 마음 이 들 었 나 봐요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 집 에 가 있 으면 좋 겠 네요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 213/213 [00:04<00:00, 42.80it/s, Loss 2.4360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.1978\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 것 도 좋 겠 어요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 이제 사람 은 항상 일찍 없 죠 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 마음 이 들 었 나요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 집 에 가 있 으면 좋 겠 네요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 213/213 [00:05<00:00, 40.71it/s, Loss 1.8961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.0784\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 걸 물 어 보 는 것 도 좋 아요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 그렇 기 도 해요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 같 은 이해 하 면 돼요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 집 에서 벗어나 면 돼요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 213/213 [00:04<00:00, 42.68it/s, Loss 1.3906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.0589\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 것 같 은가 봐요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 다음 에 는 게 좋 죠 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 같 은 이해 해 보 세요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 안 하 며 많이 해서 보 세요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 213/213 [00:04<00:00, 42.61it/s, Loss 1.0861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.0511\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 사람 이 에요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 다음 에 는 게 사람 아픈 것 처럼 도움 이 죠 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 마음 은 더 힘들 거 죠 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 세상 에 는 사람 인가 봐요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 213/213 [00:05<00:00, 41.08it/s, Loss 0.8976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 2.9818\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 하 는 걸 만나 봐요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 그렇게 생각 하 게 될 거 예요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 이해 해 보 세요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 집 에서 이랑 같이 있 는지 벗어나 세요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 213/213 [00:05<00:00, 40.80it/s, Loss 0.7520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 3.0349\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 아 먼저 생각 해 보 세요 . <end>\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 그것 일 함 이 남 죠 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 조금 만 생각 해 보 세요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 안 같이 있 고 싶 네요 . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 213/213 [00:05<00:00, 40.81it/s, Loss 0.6610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss 2.9380\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 좋 는다면 후회 하 고 있 다면 먼저 먼저 말 해 보 세요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 다음 에 는 잊어버리 는 게 습관 이 라고 생각 하 세요 . <end>\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 새로운 걸 표현 하 세요 . <end>\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 세상 의 같이 듣 고 싶 네요 . <end>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "transformer.train(\n",
    "    model,\n",
    "    dataset,\n",
    "\n",
    "    EPOCH=10,\n",
    "    enc_tokenizer=tokenizer,\n",
    "    dec_tokenizer=tokenizer,\n",
    "    example_sentence=example_sentence,\n",
    "    type=\"mecab\",\n",
    "    show_translate=True,\n",
    "    plot_attention=False,\n",
    "    valid_data=dataset_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 일찍 일어났더니 피곤하다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'다음 에 는 잊어버리 는 게 습관 이 라고 생각 하 세요 . <end>'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_sentence[1])\n",
    "transformer.translate_mecab().translate(\n",
    "    example_sentence[1], model, tokenizer, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종합 bleu점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu 점수\n",
    "def eval_bleu_single(\n",
    "    model,\n",
    "    src_sentence,\n",
    "    tgt_sentence,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    tokeninze_type,\n",
    "    max_len,\n",
    "    verbose=True,\n",
    "):\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "    if tokeninze_type == \"sp\":\n",
    "        src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "        tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "        translator = transformer.translate_sp()\n",
    "    elif tokeninze_type == \"mecab\":\n",
    "        src_tokens = src_tokenizer.texts_to_sequences(src_sentence.split(\" \"))\n",
    "        tgt_tokens = tgt_tokenizer.texts_to_sequences(tgt_sentence.split(\" \"))\n",
    "\n",
    "        translator = transformer.translate_mecab()\n",
    "\n",
    "    if len(src_tokens) > max_len:\n",
    "        return None\n",
    "    if len(tgt_tokens) > max_len:\n",
    "        return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    # 토큰 제거\n",
    "    reference = reference[1:-1]\n",
    "\n",
    "    candidate = translator.translate(\n",
    "        src_sentence, model, src_tokenizer, tgt_tokenizer\n",
    "    ).split()\n",
    "    # 토큰 제거\n",
    "    candidate = candidate[:-1]\n",
    "    score = sentence_bleu(\n",
    "        [reference], candidate, smoothing_function=SmoothingFunction().method1\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def eval_bleu(\n",
    "    model,\n",
    "    src_sentences,\n",
    "    tgt_sentence,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    tokeninze_type,\n",
    "    max_len,\n",
    "    verbose=True,\n",
    "):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(\n",
    "            model,\n",
    "            src_sentences[idx],\n",
    "            tgt_sentence[idx],\n",
    "            src_tokenizer,\n",
    "            tgt_tokenizer,\n",
    "            tokeninze_type,\n",
    "            max_len,\n",
    "            verbose,\n",
    "        )\n",
    "        if not score:\n",
    "            continue\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_test_text = tokenizer.sequences_to_texts(enc_test)\n",
    "dec_test_text = tokenizer.sequences_to_texts(dec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  친구 도 연인 도 아닌 그런 사이 의 끝\n",
      "Model Prediction:  ['후유증', '이', '클', '수', '도', '있', '어요', '.']\n",
      "Real:  ['후유증', '이', '클', '수', '도', '있', '겠', '어요', '.']\n",
      "Score: 0.675292\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6752918218126556"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bleu_single(\n",
    "    model, enc_test_text[0], dec_test_text[0], tokenizer, tokenizer, \"mecab\", 50, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1513/1513 [16:26<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 1513\n",
      "Total Score: 0.08246267707338589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(\n",
    "    model,\n",
    "    enc_test_text,\n",
    "    dec_test_text,\n",
    "    tokenizer,\n",
    "    tokenizer,\n",
    "    \"mecab\",\n",
    "    100,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoder로 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() 구현\n",
    "def beam_search_decoder(\n",
    "    sentence, src_len, tgt_len, model, src_tokenizer, tgt_tokenizer, beam_size\n",
    "):\n",
    "    def calc_prob(src_ids, tgt_ids, model):\n",
    "        enc_padding_mask, enc_dec_mask, dec_padding_mask = transformer.generate_masks(\n",
    "            src_ids, tgt_ids\n",
    "        )\n",
    "\n",
    "        predictions, _, _, _ = model(\n",
    "            src_ids, tgt_ids, enc_padding_mask, enc_dec_mask, dec_padding_mask\n",
    "        )\n",
    "        return tf.math.softmax(predictions, axis=-1)\n",
    "\n",
    "    tokens = src_tokenizer.texts_to_sequences(sentence.split(\" \"))\n",
    "\n",
    "    tokens = [x for token in tokens for x in token]\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [tokens], maxlen=src_len, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size,), dtype=np.int64)\n",
    "    scores = np.ones((beam_size,))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.word_index[\"<start>\"]\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size,))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx * beam_size\n",
    "\n",
    "            score_cache[cache_pos : cache_pos + beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos : cache_pos + beam_size, :seq_pos] = pred_tmp[\n",
    "                branch_idx, :seq_pos\n",
    "            ]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx * beam_size\n",
    "\n",
    "            if seq_pos != 1:  # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos + beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos + beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1:\n",
    "                continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, : seq_pos + 1]\n",
    "\n",
    "            pred_tmp[beam_idx, : seq_pos + 1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.word_index[\"<end>\"]:\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        # zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n",
    "        # short_pred = long_pred[:zero_idx+1]\n",
    "        # pred.append(short_pred)\n",
    "        pred.append(long_pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam(reference, ids, tokenizer):\n",
    "    print(\"입력:\", reference)\n",
    "    reference = reference.split()\n",
    "\n",
    "    for i, _id in enumerate(ids):\n",
    "        # start, end 토큰과 패딩을 제외하고 한글로 변환\n",
    "        sequence = [\n",
    "            [x]\n",
    "            for x in _id\n",
    "            if x\n",
    "            not in [tokenizer.word_index[\"<start>\"], tokenizer.word_index[\"<end>\"], 0]\n",
    "        ]\n",
    "        candidate = tokenizer.sequences_to_texts(sequence)\n",
    "        candidate = \" \".join(candidate)\n",
    "\n",
    "        print(f\"{i+1}번째 답변:\", candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = beam_search_decoder(\n",
    "    enc_test_text[0],\n",
    "    max_len,\n",
    "    max_len,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    tokenizer,\n",
    "    beam_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 친구 도 연인 도 아닌 그런 사이 의 끝\n",
      "1번째 답변: 후유증 이 클 수 도 있 어요 .\n",
      "2번째 답변: 후유증 친구 클 수 도 있 어요 .\n",
      "3번째 답변: 후유증 이 클 거 도 있 어요 .\n",
      "4번째 답변: 친구 이 클 수 도 있 어요 .\n",
      "5번째 답변: 후유증 친구 클 거 도 있 어요 .\n"
     ]
    }
   ],
   "source": [
    "beam(enc_test_text[0], ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> 후유증 이 클 수 도 있 겠 어요 . <end>'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본\n",
    "dec_test_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론 및 회고\n",
    "\n",
    "모델 학습을 많이 돌려보지 않아서인지, valid loss는 3보다 조금 작을 정도 밖에 떨어지지 않았다. trainloss는 0.6까지 떨어지는 동안 과적합이 일어나지는 않았는데, 더 이상 valid가 떨어지기 힘들것 같았다. bleu는 번역이 아니고 챗봇이니까 별로 의미없다고 생각하였고, beam search가 여러 답변을 한번에 볼 수 있어서 재미있었다. 후유증을 친구랑 바꾼 것이 좋은 문장이라고 판단하는게 아주 어이가 없었지만, 지난번 챗봇보다는 괜찮다고 느낀게 일단 데이터에 있는 답변만 주지 않아서 좋았다.\n",
    "\n",
    "데이터 중복을 제거할 때, 같은 질문에 여러 답이 나오는 것을 피하고 싶은지(a의 중복), 여러 질문에 한 답이 나오는 건지(q의 중복) 잘 모르겠어서 그냥 다 지워버리니까 4000개, 1/3의 데이터가 지워져서 의문이 들었다. 요구사항이 더 명확하게 나오면 좋을 것 같다. 사실 챗봇이니까 두 상황 모두 괜찮지 않을까 생각하였는데, 어떤 것을 의도하고 중복을 제거하라는 건지 명확한 이해가 되지 않았다.  \n",
    "\n",
    "그래서 증강이 꼭 필요하였는데, lexical sub 방식은 운에 따라서 훈련 중 loss가 nan이 되버리는 현상이 있었다. 일단 lexical sub를 한 데이터가 그냥 중복으로 인식되어 그런 것 같았다. lexical을 더 많은 단어를 하거나 등의 여러 해결방안이 있었겠지만, 증강데이터를 다시 만드니 해결이 되었다. 랜덤으로 선택되는 단어가 문제가 있던거 같은데 뭐가 문제인지 정확하게 알지는 못하였다. 다른 분도 비슷한 오류가 난 걸 보니 이 데이터세트에는 lexical이 좋은 방법이 아닌 것 같았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
