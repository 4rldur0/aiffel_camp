{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77786f28",
   "metadata": {},
   "source": [
    "## klue nsmc 모델\n",
    "klue : 한국어 버전 glue   \n",
    "nsmc : Naver Sentiment Movie Corpus, 네이버 영화리뷰 감정분석\n",
    "\n",
    "#### 루브릭\n",
    "1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.\n",
    "\n",
    "    ```text\n",
    "    ***** Running training *****\n",
    "    Num examples = 150000\n",
    "    Num Epochs = 3\n",
    "    Instantaneous batch size per device = 64\n",
    "    Total train batch size (w. parallel, distributed & accumulation) = 64\n",
    "    Gradient Accumulation steps = 1\n",
    "    Total optimization steps = 7032\n",
    "    ```\n",
    "2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.  \n",
    "\n",
    "    토큰화를 진행하고 평균길이 22, 최대길이 120정도이기 떄문에 패딩을 축소.  \n",
    "    배치사이즈 증대  \n",
    "    ```python\n",
    "    tokenizer.model_max_length = 32\n",
    "    per_device_train_batch_size = 64\n",
    "    per_device_eval_batch_size = 64\n",
    "    ```  \n",
    "    ```text\n",
    "    [7032/7032 51:11, Epoch 3/3]\n",
    "    Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\n",
    "    1\t0.272700\t0.259106\t0.893620\t0.893516\n",
    "    2\t0.196600\t0.263661\t0.894220\t0.898333\n",
    "    3\t0.141600\t0.284205\t0.900300\t0.901678\n",
    "    ```\n",
    "3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.\n",
    "\n",
    "    bucketing 이전 모델도 패딩 길이를 줄여서 시간이 많이 단축되었지만,  \n",
    "    bucketing하니 시간이 더 줄었고, 성능의 저하도 있지 않았다. \n",
    "    ```python\n",
    "    datacollator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "    TrainingArguments(group_by_length = True)\n",
    "    ```\n",
    "    ```text\n",
    "    [7032/7032 50:33, Epoch 3/3]\n",
    "    Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\n",
    "    1\t0.256600\t0.238329\t0.902100\t0.902453\n",
    "    2\t0.178400\t0.241524\t0.905640\t0.906046\n",
    "    3\t0.125600\t0.276496\t0.905860\t0.907165\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8cc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eb7cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20180f19aa33405380fef30ab2725cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a0ae8cd728404c808cd2ac31ef3a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default (download: 18.62 MiB, generated: 20.90 MiB, post-processed: Unknown size, total: 39.52 MiB) to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f5fdbf145b4006bec99d0df0a65107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f44fecac1444ff867a1f9cac2db836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5b5d41b6fe4ab8955a9fec694b8ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44478eeb04924631b9f495bc71edb986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e5f09c668484184f690cb2594208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 데이터 셋\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nsmc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ef3bdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b5b712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': '아 더빙.. 진짜 짜증나네요 목소리', 'id': '9976970', 'label': 0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f356e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaa6e907f1946b391fd44cdff2d2548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bd799686f2423083da12d81ed50e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f84f60e9a9845b696b223b72cd9cc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c5b09452b947b7b692504c4fa1f017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c9d40d2f8b4308a862f9128cc9a3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e50bd070b84295ae217bb33b6004f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "375f1091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 버트는 기본적으로 패딩이 512개인데 리뷰데이터는 그렇게 길지 않음\n",
    "len(tokenizer(\"sample text\", padding=\"max_length\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a13b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 토큰화 길이\n",
    "train_documents = dataset[\"train\"][\"document\"]\n",
    "train_input_lengths = [len(tokenizer(doc)[\"input_ids\"]) for doc in train_documents]\n",
    "\n",
    "# test 토큰화하고 길이\n",
    "test_documents = dataset[\"test\"][\"document\"]\n",
    "test_input_lengths = [len(tokenizer(doc)[\"input_ids\"]) for doc in test_documents]\n",
    "\n",
    "# 평균\n",
    "avg_train_input_length = sum(train_input_lengths) / len(train_input_lengths)\n",
    "avg_test_input_length = sum(test_input_lengths) / len(test_input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6230fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Test 최대 길이:  142 122\n",
      "Train 평균 길이: 22.275513333333333\n",
      "Test 평균 길이: 22.35976\n"
     ]
    }
   ],
   "source": [
    "print(\"Train, Test 최대 길이: \", max(train_input_lengths), max(test_input_lengths))\n",
    "print(f\"Train 평균 길이: {avg_train_input_length}\")\n",
    "print(f\"Test 평균 길이: {avg_test_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "188cdd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 32\n",
    "len(tokenizer(\"sample text\", padding=\"max_length\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e55b4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        text=data[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_token_type_ids=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f727b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3df80648f84651b1ef818aac5a520a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201a65c2a0594904a14cdc1b6959080d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(\n",
    "    transform,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e635f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_tokenized[\"train\"]\n",
    "test = dataset_tokenized[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2af6236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시간이 길어 데이터 축소\n",
    "subset_size = 30000\n",
    "train_subset = train.shuffle(seed=42).select(range(subset_size))\n",
    "test_subset = test.shuffle(seed=42).select(range(int(subset_size/3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415103c",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189e2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = \"model/nsmc/\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,  # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",  # evaluation하는 빈도\n",
    "    learning_rate=2e-5,  # learning_rate\n",
    "    per_device_train_batch_size=64,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,  # evaluation 시에 batch size\n",
    "    num_train_epochs=3,  # train 시킬 총 epochs\n",
    "    weight_decay=0.01,  # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "457a2519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff606c0f32db4168a9974971dd9fb9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정확도\n",
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')   # 바이너리 크로스엔트로피\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a0b4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7032' max='7032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7032/7032 51:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.259106</td>\n",
       "      <td>0.893620</td>\n",
       "      <td>0.893516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.263661</td>\n",
       "      <td>0.894220</td>\n",
       "      <td>0.898333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.284205</td>\n",
       "      <td>0.900300</td>\n",
       "      <td>0.901678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/nsmc/checkpoint-500\n",
      "Configuration saved in model/nsmc/checkpoint-500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-1000\n",
      "Configuration saved in model/nsmc/checkpoint-1000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-1500\n",
      "Configuration saved in model/nsmc/checkpoint-1500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-2000\n",
      "Configuration saved in model/nsmc/checkpoint-2000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model/nsmc/checkpoint-2500\n",
      "Configuration saved in model/nsmc/checkpoint-2500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-3000\n",
      "Configuration saved in model/nsmc/checkpoint-3000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-3500\n",
      "Configuration saved in model/nsmc/checkpoint-3500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-4000\n",
      "Configuration saved in model/nsmc/checkpoint-4000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-4500\n",
      "Configuration saved in model/nsmc/checkpoint-4500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-4500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model/nsmc/checkpoint-5000\n",
      "Configuration saved in model/nsmc/checkpoint-5000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-5500\n",
      "Configuration saved in model/nsmc/checkpoint-5500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-6000\n",
      "Configuration saved in model/nsmc/checkpoint-6000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-6500\n",
      "Configuration saved in model/nsmc/checkpoint-6500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-7000\n",
      "Configuration saved in model/nsmc/checkpoint-7000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-7000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7032, training_loss=0.21353977382928982, metrics={'train_runtime': 3072.1841, 'train_samples_per_second': 146.476, 'train_steps_per_second': 2.289, 'total_flos': 7399998432000000.0, 'train_loss': 0.21353977382928982, 'epoch': 3.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # 학습시킬 model\n",
    "    args=training_arguments,  # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train,  # training dataset\n",
    "    eval_dataset=test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ded8bc",
   "metadata": {},
   "source": [
    "### Bucketing\n",
    "\n",
    "Data Collator를 사용해서 Bucketing과  dynamic padding 구현 후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be424a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩을 안하고 토크나이징 \n",
    "def transform_bucket(data):\n",
    "    return tokenizer(\n",
    "        text=data[\"document\"],\n",
    "        return_token_type_ids=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c1127f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fefba4d51a5456c9b3bc85d9089ba6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c042ce9dc47b4ac9b9e8356d08efc6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_bucket = dataset.map(\n",
    "    transform_bucket,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ef8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bucket = dataset_bucket['train']\n",
    "test_bucket = dataset_bucket['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa95e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "datacollator = DataCollatorWithPadding(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd9e1448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments_bucket = TrainingArguments(\n",
    "    output_dir,  # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",  # evaluation하는 빈도\n",
    "    learning_rate=2e-5,  # learning_rate\n",
    "    per_device_train_batch_size=64,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,  # evaluation 시에 batch size\n",
    "    num_train_epochs=3,  # train 시킬 총 epochs\n",
    "    weight_decay=0.01,  # weight decay\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4866c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7032' max='7032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7032/7032 50:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.902453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.241524</td>\n",
       "      <td>0.905640</td>\n",
       "      <td>0.906046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.276496</td>\n",
       "      <td>0.905860</td>\n",
       "      <td>0.907165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/nsmc/checkpoint-500\n",
      "Configuration saved in model/nsmc/checkpoint-500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-1000\n",
      "Configuration saved in model/nsmc/checkpoint-1000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-1500\n",
      "Configuration saved in model/nsmc/checkpoint-1500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-2000\n",
      "Configuration saved in model/nsmc/checkpoint-2000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model/nsmc/checkpoint-2500\n",
      "Configuration saved in model/nsmc/checkpoint-2500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-3000\n",
      "Configuration saved in model/nsmc/checkpoint-3000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-3500\n",
      "Configuration saved in model/nsmc/checkpoint-3500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-4000\n",
      "Configuration saved in model/nsmc/checkpoint-4000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-4500\n",
      "Configuration saved in model/nsmc/checkpoint-4500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-4500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model/nsmc/checkpoint-5000\n",
      "Configuration saved in model/nsmc/checkpoint-5000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-5500\n",
      "Configuration saved in model/nsmc/checkpoint-5500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-6000\n",
      "Configuration saved in model/nsmc/checkpoint-6000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-6500\n",
      "Configuration saved in model/nsmc/checkpoint-6500/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to model/nsmc/checkpoint-7000\n",
      "Configuration saved in model/nsmc/checkpoint-7000/config.json\n",
      "Model weights saved in model/nsmc/checkpoint-7000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7032, training_loss=0.18966671400645216, metrics={'train_runtime': 3035.1956, 'train_samples_per_second': 148.261, 'train_steps_per_second': 2.317, 'total_flos': 5453371288919040.0, 'train_loss': 0.18966671400645216, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_bucket = Trainer(\n",
    "    model=model,  # 학습시킬 model\n",
    "    args=training_arguments_bucket,  # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_bucket,  # training dataset\n",
    "    eval_dataset=test_bucket,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=datacollator,\n",
    ")\n",
    "trainer_bucket.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54506f",
   "metadata": {},
   "source": [
    "###  결론 및 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82713c",
   "metadata": {},
   "source": [
    "bucketing의 경우 이번 프로젝트에서 특히나 효과적인 기법이었다. bert의 기본 패딩 사이즈가 512나 되지만, nsmc 리뷰데이터는 그 정도로 긴 문장이 없고, 토큰화를 하더라도 평균길이가 22, 최대 120이기 때문에 엄청난 절약이 있었다. 데이터 길이를 확인하고 수동으로 줄여도 되지만, bucketing을 하면 데이터 길이를 고려하지 않아도 자동적으로 되니 특히나 좋은것 같았다.  \n",
    "\n",
    "그리고 huggingface transformers 패키지가 왜 그렇게 인기가 많은지도 잘 알게 되었다. 모델 자체를 불러오는 것도 편할 뿐 아니라, 파인튜닝 과정도 전부 구현이 되어있어 바로 불러올 수 있는 것이 매우 편했다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
