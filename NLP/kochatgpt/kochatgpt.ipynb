{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF방식으로 KoChatGPT 간단히 만들어보기\n",
    "\n",
    "출처 : https://github.com/airobotlab/KoChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "1. 기존 KoGPT2와 SFT 적용 모델 결과 분석했는가?  \n",
    "sft모델의 결과물은 미세먼지를 제외하고는 부정확한 정보를 말하고 있지만, 파인튜닝을 하기 전 base모델은 단순히 다음 단어를 예측해서 이야기를 만드는 느낌이 강했다면, sft는 어느정도 답변을 하는 형식이 되었다. \n",
    "    ```text\n",
    "    * 기존 KoGPT2\n",
    "\n",
    "    불고기용 고기 한우에요??\n",
    "    우유 한 잔씩 먹었는데도 괜찮더라구요.\n",
    "    예전에 요번에도 맛있게 먹\n",
    "\n",
    "    리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "    이라고 대답해 눈길을 끌었다.\"1997년 1월 4일 공화당 상원의원 선거에 출마한 깅리치 의\n",
    "\n",
    "    시카고 오헤어 국제공항은 어디에 있어?\"\n",
    "    \"호텔에 있어요. 아저씨, 어디서 오시려니까.\"\"여기가 바로 여기죠?\n",
    "\n",
    "    오늘 미세먼지 어때? \n",
    "    아~ 예. 그렇죠? 네. 예예. 맞아요. 그래서 어~ 그러면은 지금 우리 이~ 저~\n",
    "\n",
    "    * SFT\n",
    "\n",
    "    불고기용 고기 한우에요?\n",
    "    저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
    "\n",
    "    리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "    '리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
    "\n",
    "    시카고 오헤어 국제공항은 어디에 있어?\n",
    "    '시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
    "\n",
    "    오늘 미세먼지 어때?\n",
    "    '저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n",
    "    ```\n",
    "\n",
    "2. SFT 모델과 RM 모델 결과 분석을 해보았는가?   \n",
    "    RM 모델, RLHF을 적용한 결과인데, 성능이 좋아져진건지 그냥 봐서는 알기 힘들었다. \n",
    "    ```text\n",
    "    ### Instruction(명령어):\n",
    "    불고기용 고기 한우에요?\n",
    "\n",
    "    ### Response(응답):'일반적으로 불고기에 불고기는 불고기는 불고기나 쇠고기고기에 해당합니다. 불고기의 쇠고기는 주로 육류, 생선, 해산 등의 부위를 많이 사용합니다. 하지만 불고기는 주로 소고기, 소고기, 조개, 고기, 연두부, 송아지, 참치, 양파, 등등 다양한 부위를 사용합니다. 일반적으로 쇠고기보다는 돼지고기와 쇠고기를 주로 사용합니다.高麗, \"불고기용 고기 한우에 대한 불고기가 무엇인지에 대한 정보\" 입니다. \"불고기용이 어떤 종류의 것인지는 불고기용이 무엇인지 명확히 알려주면 됩니다.\\n\\n\\n불고기용의 불고기는 주로 육류, 채소, 소고기, 양파, 양파, 그리고 쇠고기의 부위를 사용하기 때문입니다. 그러나 불고기와 불고기는 주로 쇠고기와 마찬가지로 소고기와 함께 먹는 경우가 많습니다.\\n\\n불고기의 쇠고기의 불고기는 주로 돼지고기와는 다를 수 있습니다. 따라서 불고기와 불고기는 불고기용 고기이며, 불고기는 불고기용으로 조리하는 경우가 있습니다.\\n\\n따라서, 불고\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "\n",
    "    ### Response(응답):'리처드 닉슨은 43대 부통령직을 수행한 년도는 1952년입니다. helping you\\kn\\n1960년대 후반인 2016년입니다. helping you\\n2010년부터는 2012년까지는 34대 부통령직을 수행하였습니다. helping you\\n2014년부터는 2021년까지는 helping you\\n2021년 이후로는 부통령 자리를 대신하는 부통령직을 수행하였습니다. helping you\\n2021년부터는 helping you\\n2021년까지는 부통령 자리를 대신합니다. helping you\\n2021년 후에는 부통령직을 수행합니다. helping you\\n2020년 이후에는 helping you\\n2021년 이후부터는 helping you\\n2030년에 부통령 자리를 대신하며 대통령이 됩니다. helping you\\n2020년부터는 helping you\\n2030년에 부통령직을 수행한 후, helping y\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    시카고 오헤어 국제공항은 어디에 있어\n",
    "\n",
    "    ### Response(응답):'저는 인공지능 언어모델로써 시카고 오헤어 국제공항에 관한 정보를 알지 못합니다. 따라서 제가 답변 드리기 위해서는 구체적인 위치와 시설, 위치, 교통수단 등을 고려해 주시기 바랍니다.さつは都市新橋區新橋南区新橋市新橋區新橋新橋區區新橋區新橋區北関係有入目関話中神洞口西屋) 지역이 언급됩니다. 이 지역은 뉴욕 국제공항, 시카고 국제공항, 로스앤젤레스 국제공항 등이 있습니다. \\n:\\n그 외, 시카고 국제공항과 같은 국제공항도 있습니다.n: (신)佐 \\n:\\n: \\n:\\n- \\n- \\n- \\n: \\n- \\n- \\n- \\n미세한 지형도 있습니다. \\n-  \\n또한, 뉴욕 국제공항은 미국의 국제공항으로, htm.\\n- \\n따라서, 시카고시 오헤어 국제공항은 어디인지\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    오늘 미세먼지 어때?\n",
    "\n",
    "    ### Response(응답):'오늘 미세먼지 때문에 인해 건강에 해로움을 느끼는 경우가 많습니다. 미세먼지는 건강에 매우 위험한 영향을 미치며 환경이나 환경 조건에 따라 건강 상태가 악화될 수 있습니다. 따라서 매일 미세먼지를 씻어내고 건강 상태를 유지하는 것이 중요합니다. 또한, 외출 시 마스크를 착용하는 것이 좋습니다. 또한 환경 친화적이고 충분한 수분을 유지하기 위해서는 건강한 식습관을 유지하는 것이 중요합니다. toe home precious calcident and suspermodel, as not can first through, country to resport.. \\'미안해요.\", 'token': 279}\\'token': 317}\\'token': 77} \\'sush, in want a bigger.\", 'token': 280} and token': 180}\\'token': 174} want a bigger age lot and f\n",
    "    ```\n",
    "3. 데이터셋 정제 / 새로운 데이터셋 / foundation model 교체 중 하나를 이용해 정량적 성능 향상을 해보았는가\n",
    "\n",
    "    데이터셋 정제를 사용하였다. `/n` `//n`등의 줄바꿈문자, `한자`, rm모델 데이터의 `'token' : n `등의 불필요한 정보들을 제거하였다.   \n",
    "    LoRA도 적용시켜보았다. rank는 8로 하였고, 다양한 rank로 비교하고 싶었는데, 성능을 평가하기 애매해서 실험이 무의미할거 같아서 하지 않았다. 학습 속도는 3배정도 빠르게 되었다. \n",
    "    \n",
    "    데이터 정제를 하고 LoRA를 해서인지 LoRA의 성능 향상은 크지 않은거 같았고, 정제는 무의미한 데이터가 학습되지 않게 해서 확실히 성능이 좋아진거 같았다. \n",
    "    \n",
    "    아래는 결과물로, 영어가 있기는 하지만 그래도 위의 결과보다는 좋아진거 같다.\n",
    "    ```text\n",
    "    ### Instruction(명령어):\n",
    "    불고기용 고기 한우에요?\n",
    "\n",
    "    ### Response(응답):죄송합니다, 불고기용 고기는 건강에 좋지 않습니다. 쇠고기는 소고기와 함께 먹으면 건강에도 나쁜 영향을 줄 수 있으므로 조심하세요. 에쿠로코리아, 이글스 등 다양한 음식 재료를 사용하시면 됩니다. ai 언어모델로 제작되었습니다. quiae khos kywoh, quiae joan kihappo toyuard i mooin unicer 등등의 이름으로도 유명합니다. sheungo ki kin happo will more refrighing questions ohanesi me. quiae kai cai toyuard i meaning more refrighing. quiae gee unice, qui cai toyuard i manner, quiae joung kihappo toyuard i meaning unicer, quia\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
    "\n",
    "    ### Response(응답):리처드 닉슨이 42대 부통령직을 수행한 년도는 1950년입니다. espithin ai language werker is connective warning in the feeling seaside provide. 언급되며, 가장 최근에는 33대 부통령직을 수행한 경우도 있습니다. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin espithin espithin ai language warning s\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    시카고 오헤어 국제공항은 어디에 있어\n",
    "\n",
    "    ### Response(응답):시카고 오헤어 국제공항은 현재 운영되고 있습니다. of preferences japan limited qu.or of citiese limited는 미국에서 운영되는 공항으로, 시카고 국제공항에 위치해 있습니다. of citiese limited 는 시카고에 위치한 도시 중 하나로, 시카고 국제공항의 메인 노선이기도 합니다. assistance of friends limited sofinities limited qu. assistance of citiese limited sofinities limited는 시카고에 위치한 공항입니다. of citiese limited sofinities limited sofinities qu.or는 미국에서 발생한 사건으로 발생한 것입니다. 라는 용어는 이 언어에서 파생된 것으로, 시카고 국제공항의 설립자로 유명합니다. assistance of citiestion qu.or of cities qu.or 는 시카고 국제공항을 운영하는 것으로 유명합니다. of citiesia qu.or (jpr\n",
    "\n",
    "    ### Instruction(명령어):\n",
    "    오늘 미세먼지 어때?\n",
    "\n",
    "    ### Response(응답):저는 ai 어시스턴트이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만 미세먼지 관련해서는 개인의 개인 취향에 따라 다르기 때문에, 상황에 따라 적절한 대처 방법이 있을 수 있습니다. 저는 인공지능 챗봇이기 때문에 어떤 문제가 있으셨는지에 대해 알 수 있지만, 개인적으로는 개인 건강 상태에 대해서 충분히 고려하고 대처해야할 필요가 있습니다. 또한, 호흡기 질환자의 경우 호흡기 질환을 예방하는 데에 주의해야합니다. 따라서 건강 상태가 좋지 않을 경우에는 전문 의료기관에서 상담을 받으시는 것이 가장 좋습니다. 또한, 건강한 생활 습관을 가지는 것이 중요합니다. 말씀드릴 수 없는 상황에서 미세먼지 문제에 대해 정확한 대처 방법을 찾은 것은 건강에 도움이 됩니다. 말씀드릴 수 있는 경우, 해당 상황을 인식하고 대처 방법을 찾아봅니다. 말씀드릴 수 있는 상황, 어떤 영향을 받으시겠다는 것인지 등의 상황에 따라서 적절한 대처 방법 및 대처 방법을 찾는 것이 중요합니다. 말씀드릴 수 있는 상황이라면, 정확한 대처 방법을 찾고 상황을 파악하시기 바랍니다. 말씀드린다면, 적절한 대응 방법을 찾아보는 것이 중요합니다. 말씀드릴 수 있는 상황이므로, 개인의 상황에 따라서 적절한 대응 방법에 대해 말씀\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베이스 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁불', '고기', '용', '▁고기', '▁한', '우에', '요', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"불고기용 고기 한우에요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불고기용 고기 한우에요??\n",
      "우유 한 잔씩 먹었는데도 괜찮더라구요.\n",
      "예전에 요번에도 맛있게 먹\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?이라고 대답해 눈길을 끌었다.\n",
      "\"1997년 1월 4일 공화당 상원의원 선거에 출마한 깅리치 의\n",
      "시카고 오헤어 국제공항은 어디에 있어?\"\n",
      "\"호텔에 있어요. 아저씨, 어디서 오시려니까.\"\n",
      "\"여기가 바로 여기죠?\n",
      "오늘 미세먼지 어때? 아~ 예. 그렇죠? 네. 예예. 맞아요. 그래서 어~ 그러면은 지금 우리 이~ 저~\n"
     ]
    }
   ],
   "source": [
    "max_length = 32\n",
    "\n",
    "list_prompt = [\n",
    "    \"불고기용 고기 한우에요?\",\n",
    "    \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\",\n",
    "    \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "    \"오늘 미세먼지 어때?\",\n",
    "]\n",
    "for l in list_prompt:\n",
    "\n",
    "    input_ids = tokenizer(l, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    output_beam = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=10,  # 빔서치 디코딩\n",
    "        no_repeat_ngram_size=2,  # n-그램\n",
    "        do_sample=True,\n",
    "        temperature=2.0,\n",
    "        top_k=50,  # 확률이 높은 k개 중 선택\n",
    "        # top_p=0.90,     # 각 토큰의 확률이 p 이상인 것 선택\n",
    "    )\n",
    "    print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SFT 데이터\n",
    "data_path_1_SFT = \"data_kochatgpt/kochatgpt_1_SFT.jsonl\"\n",
    "with open(data_path_1_SFT, \"r\", encoding=\"utf-8-sig\") as json_file:\n",
    "    sft = json.load(json_file)\n",
    "\n",
    "print(len(sft))\n",
    "sft[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RM 데이터\n",
    "data_path_2_RM = \"data_kochatgpt/kochatgpt_2_RM.jsonl\"\n",
    "with open(data_path_2_RM, \"r\", encoding=\"utf-8-sig\") as json_file:\n",
    "    RM = json.load(json_file)\n",
    "\n",
    "print(len(RM))\n",
    "RM[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 강화학습 데이터\n",
    "data_path_3_PPO = \"data_kochatgpt/kochatgpt_3_PPO.jsonl\"\n",
    "with open(data_path_3_PPO, \"r\", encoding=\"utf-8-sig\") as json_file:\n",
    "    rl = json.load(json_file)\n",
    "\n",
    "print(len(rl))\n",
    "rl[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 및 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"</s>\",\n",
    "    pad_token=\"</s>\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터\n",
    "\n",
    "토크나이저에 패딩 토큰이 1인데 왜 데이터를 불러올 때 -100으로 패딩 자리를 채우는지 모르겠음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path_1_SFT: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        verbose=False,\n",
    "        preprocess= False\n",
    "    ):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = \"prompt\"  # instruction\n",
    "        pattern_output = \"completion\"  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding=\"utf-8-sig\") as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if preprocess:\n",
    "                example['prompt'] = preprocess_text(example['prompt'])\n",
    "                example['completion'] = preprocess_text(example['completion'])\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100  # 패딩\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\" % (len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(\n",
    "        self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer\n",
    "    ) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "            for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple(\n",
    "            [instance[key] for instance in instances] for key in (\"input_ids\", \"labels\")\n",
    "        )\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=-100\n",
    "        )\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(\n",
    "    data_path_1_SFT=\"data_kochatgpt/kochatgpt_1_SFT.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print(\"input : %s\" % train_dataset.input_ids[0])\n",
    "print(\"output: %s\" % train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset.input_ids[0]))\n",
    "\n",
    "# 패딩을 어떻게 디코드 하는지?\n",
    "# print(tokenizer.decode(train_dataset.labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"output_1_SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"output_1_SFT\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375,  # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\")\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    \"불고기용 고기 한우에요?\",\n",
    "    \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\",\n",
    "    \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "    \"오늘 미세먼지 어때?\",\n",
    "]\n",
    "\n",
    "list_prompt = [\n",
    "    PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": tmp}) for tmp in list_prompt\n",
    "]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0][\"generated_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sft모델의 결과물은 미세먼지를 제외하고는 부정확한 정보를 말하고 있지만, 파인튜닝을 하기 전 base모델에 비하면 챗봇스럽게 대답을 하고 있는걸 볼 수 있었다. \n",
    "\n",
    "```text\n",
    "불고기용 고기 한우에요??\n",
    "우유 한 잔씩 먹었는데도 괜찮더라구요.\n",
    "예전에 요번에도 맛있게 먹\n",
    "리처드 닉슨이 43대 부통령직을 수행한 년도는?이라고 대답해 눈길을 끌었다.\n",
    "\"1997년 1월 4일 공화당 상원의원 선거에 출마한 깅리치 의\n",
    "시카고 오헤어 국제공항은 어디에 있어?\"\n",
    "\"호텔에 있어요. 아저씨, 어디서 오시려니까.\"\n",
    "\"여기가 바로 여기죠?\n",
    "오늘 미세먼지 어때? 아~ 예. 그렇죠? 네. 예예. 맞아요. 그래서 어~ 그러면은 지금 우리 이~ 저~\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 라이브러리 경로 추가\n",
    "sys.path.append(os.path.dirname('colossalai_ChatGPT_230319/chatgpt'))\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: Optional[str] = None,\n",
    "        config: Optional[GPT2Config] = None,\n",
    "        checkpoint: bool = False,\n",
    "        lora_rank: int = 0,\n",
    "        lora_train_bias: str = \"none\",\n",
    "        tokenizer=None,\n",
    "    ) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)    # 하나의 점수값 리턴 \n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"</s>\",\n",
    "    pad_token=\"</s>\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "    model = GPTRM_custom(\n",
    "        pretrained=\"skt/kogpt2-base-v2\", lora_rank=0, tokenizer=tokenizer\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "    \"data_kochatgpt/kochatgpt_2_RM.jsonl\", \"r\", encoding=\"utf-8-sig\"\n",
    ") as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
    "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
    "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_2\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_2\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
    "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_2\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_2\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print(\"before data num: %d\" % (len(list_data_dict)))\n",
    "print(\"after  data num: %d\" % (len(total_data_ranking2chosen)))\n",
    "print(\"data example: \\n%s\" % total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '이 빵은 혹시 며칠까지 먹을 수 있을까요?', 'chosen': '제가는 이 질문에 대한 정확한 대답을 할 수 없습니다. 이 빵의 유통기한이나 보관 방법에 따라 달라질 수 있기 때문입니다. 빵을 구매할 때는 유통기한을 확인하고, 보관 방법을 잘 따라서 하시는 것이 좋습니다. 일반적으로 빵은 냉장고에서 최대 3-4일까지 보관이 가능하며, 냉동 보관을 하면 더 오래 보관이 가능합니다. 하지만 유통기한을 확인하여 적절하게 보관하시기 바랍니다.', 'rejected': '이상 같은 국가들의 개선 개선들 개선 개선 개선\\n\\n\\n\\n이상 같은 국가들의 개선 개선 개선'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1044.04it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1139.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(\n",
    "    model=model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=Adam(model.parameters(), lr=5e-5),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    batch_size=4,\n",
    "    max_epochs=1,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained(\"output_2_RM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 불러오기 \n",
    "# model = AutoModelForCausalLM.from_pretrained('output_2_RM').to('cuda')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"skt/kogpt2-base-v2\",\n",
    "#     bos_token=\"</s>\",\n",
    "#     eos_token=\"</s>\",\n",
    "#     unk_token=\"</s>\",\n",
    "#     pad_token=\"</s>\",\n",
    "#     padding_side=\"right\",\n",
    "#     model_max_length=512,\n",
    "# )\n",
    "\n",
    "# with NaiveStrategy().model_init_context():\n",
    "#     model = GPTRM_custom(\n",
    "#         pretrained=\"output_2_RM\", lora_rank=0, tokenizer=tokenizer\n",
    "#     ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\n",
    "        torch.cuda.current_device()\n",
    "    )\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print(\"input: %s\\nreward score: %.1f\" % (input_text, output_reward))\n",
    "\n",
    "    return output_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.1\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이스 모델과 sft 모델 결과 점수보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"뭐야! 뭐라구! 그냥 놔두는 게 낫긴 한데 말이야.\" \"맞아\"\n",
      "reward score: -0.5\n",
      "input: 1971년 7월 10일, 레이건 대통령이 취임, 취임식을 거행했다. 존 F. 케네디는 취임\n",
      "reward score: -0.1\n",
      "input: \"그래? 그럼 여기 이 공항으로 오는 게 좋겠네.\" 어디까지 갔냐고 물었\n",
      "reward score: -0.4\n",
      "input: 그쵸. 어떨까 생각해 볼 수 있겠네요. 맞아요. 어~ 사실 그~ 우리 나라 공기질 네.\n",
      "reward score: -0.6\n",
      "input: 저는 인공지능 어시스턴트이기 때문에 불고기의 종류와 양에 대한 정보를 알 수 없습니다. 하지만 일반적으로 불고기는 쇠고기, 돼지고기, 닭고기, 오리고기 등 다양한 요리에 사용됩니다. 따라서 불고기를 판매하는 가게나 상점에 문의하시면 보다 정확한 정보를 얻으실 수 있습니다.\n",
      "reward score: 0.2\n",
      "input: 리처드 닉슨은 42대 부통령직을 수행했습니다. 『Circuit of the President』라고도 불립니다. 『The Records of the Ministry』라고도 불릴 수 있습니다. 『Advanced Operating』이라고도 불립니다.\n",
      "reward score: 0.1\n",
      "input: 시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.明使: \"시카고오헤어 국제공항\"이라는 뜻입니다.銘使: \"이름\", \"죄송합니다, 저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다.\"\n",
      "reward score: 0.3\n",
      "input: 저는 인공지능 챗봇이므로 미세먼지 여부를 알 수 없습니다. 하지만 일반적으로 미세먼지 농도가 높은 날에는 실외활동을 자제하는 것이 좋습니다. 또한, 미세먼지가 심한 날에는 마스크를 착용하고 실내에서 대기오염을 최소화하는 것이 좋습니다. 또한 외출 시 마스크를 착용하는 것도 도움이\n",
      "reward score: 0.3\n",
      "base 모델 평균 -0.38738328218460083\n",
      "sft 모델 평균 0.2368149608373642\n"
     ]
    }
   ],
   "source": [
    "base_output = ['\"뭐야! 뭐라구! 그냥 놔두는 게 낫긴 한데 말이야.\" \"맞아\"', \n",
    "               '1971년 7월 10일, 레이건 대통령이 취임, 취임식을 거행했다. 존 F. 케네디는 취임',\n",
    "              '\"그래? 그럼 여기 이 공항으로 오는 게 좋겠네.\" 어디까지 갔냐고 물었',\n",
    "              '그쵸. 어떨까 생각해 볼 수 있겠네요. 맞아요. 어~ 사실 그~ 우리 나라 공기질 네.']\n",
    "sft_output = ['저는 인공지능 어시스턴트이기 때문에 불고기의 종류와 양에 대한 정보를 알 수 없습니다. 하지만 일반적으로 불고기는 쇠고기, 돼지고기, 닭고기, 오리고기 등 다양한 요리에 사용됩니다. 따라서 불고기를 판매하는 가게나 상점에 문의하시면 보다 정확한 정보를 얻으실 수 있습니다.',\n",
    "             '리처드 닉슨은 42대 부통령직을 수행했습니다. 『Circuit of the President』라고도 불립니다. 『The Records of the Ministry』라고도 불릴 수 있습니다. 『Advanced Operating』이라고도 불립니다.',\n",
    "             '시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.明使: \"시카고오헤어 국제공항\"이라는 뜻입니다.銘使: \"이름\", \"죄송합니다, 저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다.\"',\n",
    "             '저는 인공지능 챗봇이므로 미세먼지 여부를 알 수 없습니다. 하지만 일반적으로 미세먼지 농도가 높은 날에는 실외활동을 자제하는 것이 좋습니다. 또한, 미세먼지가 심한 날에는 마스크를 착용하고 실내에서 대기오염을 최소화하는 것이 좋습니다. 또한 외출 시 마스크를 착용하는 것도 도움이']\n",
    "base_score = []\n",
    "sft_score = []\n",
    "for i in base_output:\n",
    "    base_score.append(inference_RM(input_text=i))\n",
    "for j in sft_output:\n",
    "    sft_score.append(inference_RM(input_text=j))\n",
    "    \n",
    "print(f'base 모델 평균 {np.mean(base_score)}')\n",
    "print(f'sft 모델 평균 {np.mean(sft_score)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.83s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000849]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.93it/s, actor_loss=0, critic_loss=0.000849]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.93it/s, actor_loss=0, critic_loss=0.315]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0, critic_loss=0.315]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0, critic_loss=0.00996]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s, actor_loss=0, critic_loss=0.00996]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:19<00:00,  6.36s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.92s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.253, critic_loss=0.0733]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.253, critic_loss=0.0733]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.29, critic_loss=0.177]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.29, critic_loss=0.177]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.257, critic_loss=0.106]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.257, critic_loss=0.106]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.49s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.08s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.143, critic_loss=0.027]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.79it/s, actor_loss=0.143, critic_loss=0.027]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.79it/s, actor_loss=0.138, critic_loss=0.00435]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=0.138, critic_loss=0.00435]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=0.147, critic_loss=0.0459] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s, actor_loss=0.147, critic_loss=0.0459]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:20<00:00,  6.71s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.285, critic_loss=0.0772]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.285, critic_loss=0.0772]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.275, critic_loss=0.0677]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.275, critic_loss=0.0677]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.281, critic_loss=0.0384]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.281, critic_loss=0.0384]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.56s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.93s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0567, critic_loss=0.00527]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.0567, critic_loss=0.00527]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.0605, critic_loss=0.0147] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.0605, critic_loss=0.0147]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.061, critic_loss=0.0339] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.061, critic_loss=0.0339]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.01s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.203, critic_loss=0.0585]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=0.203, critic_loss=0.0585]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=0.212, critic_loss=0.0307]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.212, critic_loss=0.0307]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.191, critic_loss=0.00872]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0.191, critic_loss=0.00872]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.09s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-2.54e-6, critic_loss=0.00431]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=-2.54e-6, critic_loss=0.00431]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=0.00561, critic_loss=0.00652] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.00561, critic_loss=0.00652]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.0324, critic_loss=0.0329]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.0324, critic_loss=0.0329]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.10s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.165, critic_loss=0.0264]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=-.165, critic_loss=0.0264]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=-.154, critic_loss=0.0109]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.154, critic_loss=0.0109]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.137, critic_loss=0.00597]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.137, critic_loss=0.00597]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.61s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.03s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.00304, critic_loss=0.00109]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=0.00304, critic_loss=0.00109]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=-.00169, critic_loss=0.00856]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.00169, critic_loss=0.00856]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.000227, critic_loss=0.0162]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.000227, critic_loss=0.0162]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:19<00:00,  6.64s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.09s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.112, critic_loss=0.0201]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s, actor_loss=0.112, critic_loss=0.0201]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.81it/s, actor_loss=0.135, critic_loss=0.0108]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.135, critic_loss=0.0108]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.112, critic_loss=0.00132]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=0.112, critic_loss=0.00132]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:19<00:00,  6.64s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'일반적으로 불고기에 불고기는 불고기는 불고기나 쇠고기고기에 해당합니다. 불고기의 쇠고기는 주로 육류, 생선, 해산 등의 부위를 많이 사용합니다. 하지만 불고기는 주로 소고기, 소고기, 조개, 고기, 연두부, 송아지, 참치, 양파, 등등 다양한 부위를 사용합니다. 일반적으로 쇠고기보다는 돼지고기와 쇠고기를 주로 사용합니다.高麗, \"불고기용 고기 한우에 대한 불고기가 무엇인지에 대한 정보\" 입니다. \"불고기용이 어떤 종류의 것인지는 불고기용이 무엇인지 명확히 알려주면 됩니다.\\n\\n\\n불고기용의 불고기는 주로 육류, 채소, 소고기, 양파, 양파, 그리고 쇠고기의 부위를 사용하기 때문입니다. 그러나 불고기와 불고기는 주로 쇠고기와 마찬가지로 소고기와 함께 먹는 경우가 많습니다.\\n\\n불고기의 쇠고기의 불고기는 주로 돼지고기와는 다를 수 있습니다. 따라서 불고기와 불고기는 불고기용 고기이며, 불고기는 불고기용으로 조리하는 경우가 있습니다.\\n\\n따라서, 불고\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 43대 부통령직을 수행한 년도는 1952년입니다. helping you\\kn\\n1960년대 후반인 2016년입니다. helping you\\n2010년부터는 2012년까지는 34대 부통령직을 수행하였습니다. helping you\\n2014년부터는 2021년까지는 helping you\\n2021년 이후로는 부통령 자리를 대신하는 부통령직을 수행하였습니다. helping you\\n2021년부터는 helping you\\n2021년까지는 부통령 자리를 대신합니다. helping you\\n2021년 후에는 부통령직을 수행합니다. helping you\\n2020년 이후에는 helping you\\n2021년 이후부터는 helping you\\n2030년에 부통령 자리를 대신하며 대통령이 됩니다. helping you\\n2020년부터는 helping you\\n2030년에 부통령직을 수행한 후, helping y\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'저는 인공지능 언어모델로써 시카고 오헤어 국제공항에 관한 정보를 알지 못합니다. 따라서 제가 답변 드리기 위해서는 구체적인 위치와 시설, 위치, 교통수단 등을 고려해 주시기 바랍니다.さつは都市新橋區新橋南区新橋市新橋區新橋新橋區區新橋區新橋區北関係有入目関話中神洞口西屋) 지역이 언급됩니다. 이 지역은 뉴욕 국제공항, 시카고 국제공항, 로스앤젤레스 국제공항 등이 있습니다. \\n:\\n그 외, 시카고 국제공항과 같은 국제공항도 있습니다.n: (신)佐 \\n:\\n: \\n:\\n- \\n- \\n- \\n: \\n- \\n- \\n- \\n미세한 지형도 있습니다. \\n-  \\n또한, 뉴욕 국제공항은 미국의 국제공항으로, htm.\\n- \\n따라서, 시카고시 오헤어 국제공항은 어디인지\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'오늘 미세먼지 때문에 인해 건강에 해로움을 느끼는 경우가 많습니다. 미세먼지는 건강에 매우 위험한 영향을 미치며 환경이나 환경 조건에 따라 건강 상태가 악화될 수 있습니다. 따라서 매일 미세먼지를 씻어내고 건강 상태를 유지하는 것이 중요합니다. 또한, 외출 시 마스크를 착용하는 것이 좋습니다. 또한 환경 친화적이고 충분한 수분을 유지하기 위해서는 건강한 식습관을 유지하는 것이 중요합니다. toe home precious calcident and suspermodel, as not can first through, country to resport.. \\'미안해요.\", 'token': 279}\\'token': 317}\\'token': 77} \\'sush, in want a bigger.\", 'token': 280} and token': 180}\\'token': 174} want a bigger age lot and f\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리\n",
    "```text\n",
    "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
    "```\n",
    "sft 출력 결과인 위를 보면 줄바꿈 문자와 기호, 한자 등을 학습한 것을 볼 수 있지만, 불필요하다고 생각되어 제거  \n",
    "영어 데이터 역시 없애고 싶지만, 고유명사등을 다 영어인 상태이므로 유지\n",
    "\n",
    "처음 전처리 했는데 결과가 아래와 같이 나와서 `token 22270 , tok 22370`등을 제거. \n",
    "rm데이터에 이런 데이터가 존재\n",
    "```text\n",
    "### Response(응답): 미세먼지 예방을 위해서는 생활 속 거리 두기와 미세먼지 농도를 높이는 것이 중요합니다. 만약 미세먼지가 심한 경우 보건용 마스크나 보호 장비 등을 착용해야합니다. 또한, 외출 시 미세먼지가 심한 곳은 되도록이면 마스크나 에어컨을 충분히 사용하고 실내에서 작업을 하면 좋습니다. 또한 실외에서 먼지를 제거하거나 먼지 농도를 낮추는 등 일상적인 환경 유지 노력이 필요합니다. 이 글에서 언급된 것처럼 미세먼지 예방을 위해 노력을 기울여야 한다는 것을 나타내는 문장입니다. 언급된 내용에서는 주로 미세먼지와 먼지에 대한 대비책이 구체적으로 제시되고 있습니다. 하지만 일부 전문가는 이러한 노력이 실패했다고 할 수 있습니다. 예를 들어, 마스크나 보호 장비를 사용하는 경우 실내에서 미세먼지와 먼지 농도를 줄이는 것이 매우 효과적일 수 있다고 할 수 있습니다. 그리고 대기 오염으로 인한 환경 오염이 발생했을 때 보다 적극적으로 대처해야한다는 것을 말합니다. 알려진 상황으로는 대기의 질이 나빠지는 것은 자연스런 현상이며, 이는 자연재해로 인한 직접적인 상황이나 인명 피해가 발생할 가능성을 뜻합니다., token 22270 , tok 22370  언급된\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sft 전처리  \n",
    "`SFT_dataset`, `DataCollatorForSupervisedDataset`는 위에서 실행해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"</s>\",\n",
    "    pad_token=\"</s>\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = re.sub(r\"\\s*, 'token': \\d+\\s*\", ' ', text) # , 'token' : x 라는 불필요한 정보 제거\n",
    "    text = re.sub(r'\\\\n|\\n', ' ', text)      # 줄바꿈 문자 제거\n",
    "    text = re.sub(r\"[^가-힣0-9a-zA-Z?.!,()]+\", \" \",text) # 한자,기호 제거\n",
    "    \n",
    "    text = text.lower()         \n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFT_dataset`클래스에 \n",
    "```python\n",
    "if preprocess:\n",
    "    example['prompt'] = preprocess_text(example['prompt'])\n",
    "    example['completion'] = preprocess_text(example['completion'])\n",
    "```\n",
    "추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "train_dataset_preprocess = SFT_dataset(\n",
    "    data_path_1_SFT=\"data_kochatgpt/kochatgpt_1_SFT.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    preprocess=True     # 전처리\n",
    ")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "입냄새 안나나?\n",
      "\n",
      "### Response(응답):'컴퓨터 앞에서 일하면서 입을 위해 물이나 향초를 끊임없이 찾는 이유가 여기 있군요.\\n\\n하지만 저는 인공지능 챗봇입니다. 따라서 입을 물거나 할 필요가 없으며, 입냄새도 발생하지 않습니다. 그러니 안심하고 대화를 이어 나가시면 됩니다!</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset.input_ids[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "입냄새 안나나?\n",
      "\n",
      "### Response(응답):컴퓨터 앞에서 일하면서 입을 위해 물이나 향초를 끊임없이 찾는 이유가 여기 있군요. 하지만 저는 인공지능 챗봇입니다. 따라서 입을 물거나 할 필요가 없으며, 입냄새도 발생하지 않습니다. 그러니 안심하고 대화를 이어 나가시면 됩니다!</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset_preprocess.input_ids[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.852300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset_preprocess,\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"output_1_SFT_preprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):저는 ai 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 보통 불고기용 고기는 다양한 종류의 쇠고기를 사용합니다. 예를 들어, 소고기, 돼지고기 등 다양한 종류가 있을 수 있습니다. 또한, 일부 대형마트나 슈퍼마켓에서도 불고기용 고기를 판매하고 있으니 참고하시기 바랍니다! \n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):리처드 닉슨은 46대 부통령직을 수행하지 않았습니다. espiration, personal context of the security and financial experience. educational committee on the relationships in\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):시카고 오 헤어 국제공항은 미국 텍사스 주 휴스턴에 위치해 있습니다. ai language model, i cannot provide more context of the translation service. ai response to referring inform\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):저는 인공지능 챗봇이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만, 미세먼지 예보나 관련 뉴스 등을 참고하시면 도움이 될 것 같습니다. 감사합니다. committee model, i will please provide more information or serv\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"output_1_SFT_preprocess\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375,  # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\")\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    \"불고기용 고기 한우에요?\",\n",
    "    \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\",\n",
    "    \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "    \"오늘 미세먼지 어때?\",\n",
    "]\n",
    "\n",
    "list_prompt = [\n",
    "    PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": tmp}) for tmp in list_prompt\n",
    "]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0][\"generated_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rm 전처리  \n",
    "모델은 위에서 불러와야 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"data_kochatgpt/kochatgpt_2_RM.jsonl\", \"r\", encoding=\"utf-8-sig\"\n",
    ") as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "    \n",
    "    tmp[\"prompt\"] = preprocess_text(tmp[\"prompt\"])\n",
    "    tmp[\"completion_0\"] = preprocess_text(tmp[\"completion_0\"])\n",
    "    tmp[\"completion_1\"] = preprocess_text(tmp[\"completion_1\"])\n",
    "    tmp[\"completion_2\"] = preprocess_text(tmp[\"completion_2\"])\n",
    "    \n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
    "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
    "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_2\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_2\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data[\"prompt\"] = tmp[\"prompt\"]\n",
    "    if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
    "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_2\"]\n",
    "    else:\n",
    "        data[\"chosen\"] = tmp[\"completion_2\"]\n",
    "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"before data num: %d\" % (len(list_data_dict)))\n",
    "print(\"after  data num: %d\" % (len(total_data_ranking2chosen)))\n",
    "print(\"data example: \\n%s\" % total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '이 빵은 혹시 며칠까지 먹을 수 있을까요?', 'chosen': '제가는 이 질문에 대한 정확한 대답을 할 수 없습니다. 이 빵의 유통기한이나 보관 방법에 따라 달라질 수 있기 때문입니다. 빵을 구매할 때는 유통기한을 확인하고, 보관 방법을 잘 따라서 하시는 것이 좋습니다. 일반적으로 빵은 냉장고에서 최대 3 4일까지 보관이 가능하며, 냉동 보관을 하면 더 오래 보관이 가능합니다. 하지만 유통기한을 확인하여 적절하게 보관하시기 바랍니다.', 'rejected': '이상 같은 국가들의 개선 개선들 개선 개선 개선 이상 같은 국가들의 개선 개선 개선'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1339.71it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1353.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset_preprocess = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset_preprocess = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(\n",
    "    model=model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=Adam(model.parameters(), lr=5e-5),\n",
    "    train_dataset=train_dataset_preprocess,\n",
    "    eval_dataset=eval_dataset_preprocess,\n",
    "    batch_size=4,\n",
    "    max_epochs=1,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained(\"output_2_RM_preprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ppo의 경우 질문이 깔끔하기에 전처리는 할 필요가 없음  \n",
    "훈련만 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='output_1_SFT_preprocess', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='output_2_RM_preprocess', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
    "    \n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('output_3_PPO_preprocess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 결과\n",
    "\n",
    "보다 깔끔한 결과가 나왔지만, 영어로 변하는 문제와 조금의 한자는 학습을 한 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):저는 인공지능 언어모델이며, 어떤 종류의 고기에 대해 알려주시려는지 명확하지 않습니다. 하지만 일반적으로는 불고기를 판매하기 전에 일반적으로 냉동 후 오븐이나 양념된 고기, 된장 등에 포함된 고기를 선택하시면 됩니다. 따라서 불고기가 얼마나 좋으신가요? 라고 말씀드릴 수 있습니다. 따라서, 인터넷에 검색하시면 더 자세한 정보를 얻을 수 있을 것입니다. 감사합니다. 辛)의 고기용 고기는 보통 한우의 냉동실에 보관하는 경향이 있는데, 불고기는 일반적으로 오븐에 담아 보관합니다. 따라서 해당 식당에 직접 문의하시거나, 인터넷에 검색해보시는 것이 좋을 것입니다. 辛의 고기를 선택하시길 바랍니다. 辛의 고기는 주로 소고기의 냉동고기와 동일한 종류이고, 보통 한우 또는 된장불고기의 냉동고에 보관합니다. 辛의 고기용 고기는 일반적으로 한우의 냉동고기와 같은 종류이며, 가격도 다를 수 있습니다. 辛의 고기용 고기는 주로 쇠고기의 경우 한우의 냉동실에 보관하는 경우가 많지만, 한우의 냉장고에 보관하는 것이 일반적입니다. 辛의 고기용 고\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):리처드 닉슨은 41대 부통령직을 수행한 경력이 없습니다. 정치학자인 에드몬드 앤더슨이 1978년 부통령직을 수행하면서 부통령으로 임명되었습니다. 법학 분야에서 활동한 그는 주로 사법부문에서 두각을 나타내면서 대통령 선거, 대법원 등에서 많은 영향력을 미쳤습니다. 법학 교수로서 존경과 의무를 보인 것을 높이 평가받았습니다. 정치학자가 아닌 정치학자로써 대통령 선거, 대법원 판결, 대통령 선거 등 주요 행정 분야에서 활동하고 있습니다. 정치학자가 아닌 정치학자로서 대통령 선거를 관리하는 역할을 맡았으며 대통령 선거 때 다수의 언론인과 대등하게 싸웠습니다. 법학 분야의 학문적인 분석과 함께 존경과 의무를 다하여 수많은 업적을 남겼습니다. 법학 교수로서 존경과 의무를 다함으로써 대통령으로 재직하며 대통령 선거에서 중요한 역할을 하였습니다. 법학 교수로서 존경과 의무를 다하며 대통령이 아니라 대통령으로서 대통령의 권한을 행사하며, 대통령의 권한 중 법률, 국가 및 국가의 안보, 외교정책 등의 분야에 적극적으로 참여하였습니다. 법학 교수로서 존경과 의무를 다하였으며, 대중의 존경을 받으며, 대통령으로서 국민들의 사랑을 받았습니다. 법학 교수로서 존경과 의무를 다하며 국민들의 사랑을 받아왔습니다. 윤리학\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):시카고 오헤어 국제공항은 시카고 국제공항의 일부 지역입니다. lovel mark와 관련된 정보가 있을 수 있습니다. marvel meanings of service of doomness sexarter francy of their service pushibrocess language model, civil in the pictude. lovel and service in the macrocess media in crossfrank, ya blue ushes soons, or soons, are your locaccess means education better francies with their service of the service education, if you start and please means education means to doomness in crossfrancies pushibrocity in crossfrancies in their\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):제가 ai이므로 미세먼지 여부를 판단할 수는 없습니다. 그러나 일반적으로 미세먼지 관리는 보통 환경 보호와 건강 유지에 매우 중요하기 때문에, 매년 미세먼지가 측정되고 있습니다. 따라서 미세먼지 농도가 매우 나쁜 경우에는 실외 활동을 자제하고, 실내 활동을 자제하시는 것이 좋습니다. 또한, 미세먼지 발생 시 대기 상태를 유지하고 주변 환경을 보호하기 위해 충분한 수분 섭취, 규칙적인 수면 등을 권장합니다. )는 모두 미세먼지 농도를 고려하지 않습니다. 그러나 미세먼지 제거가 필요하다 는 말은 모든 사람들이 이해할 수 있는 것이 아닙니다. 따라서 미세먼지 농도를 고려하는 것은 개인의 판단과 의사 결정은 매우 중요합니다. 은 미세먼지가 발생하지 않도록 돕는다는 것은 매우 유익하지만, 정확한 데이터를 얻으시려면 시간이 걸릴 수 있습니다. 은 대부분의 사람이 이해하기 어렵기 때문에, 개인마다 개인적인 선택에 대해 더 많은 정보와 시간을 주세요. 은 모든 사람이 이해하기 어렵기 때문에, 개인이 더 나은 선택인지 확인하신다면 보다 정확하고 효과적인 결과를 낼 수 있습니다. 은 모든 사람들이 이해하기 어려울 수 있지만, 매우 효과적인 해결책이 될 것입니다. 은 모든 사람이 이해하기 어려운 것이 아니기 때문에, 개인마다 상황과 특성을 고려한 정확한 데이터를 수집\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora \n",
    "```python\n",
    "class LoRAModule(nn.Module):\n",
    "    \"\"\"A LoRA module base class. All derived classes should call `convert_to_lora()` at the bottom of `__init__()`.\n",
    "    This calss will convert all torch.nn.Linear layer to LoraLinear layer.\n",
    "\n",
    "    Args:\n",
    "        lora_rank (int, optional): LoRA rank. 0 means LoRA is not applied. Defaults to 0.\n",
    "        lora_train_bias (str, optional): Whether LoRA train biases.\n",
    "            'none' means it doesn't train biases. 'all' means it trains all biases. 'lora_only' means it only trains biases of LoRA layers.\n",
    "            Defaults to 'none'.\n",
    "    \"\"\"\n",
    "```\n",
    "다양한 lora rank를 적용하여 적절한 lora 적용  \n",
    "\n",
    "lora를 위한 패키지 peft는 torch 버전에 호환이 안되서 코드로 구현이 되어있는 rm과 ppo 단계에서 적용\n",
    "\n",
    "대략 3배 정도 빨랐음 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_rank = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora를 사용한 rm모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d0d1064354388b08ca31a058d2a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317c95a5431848df9e21f42c558e549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2fab75ee6b479da4226317c3080f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"</s>\",\n",
    "    pad_token=\"</s>\",\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "    model = GPTRM_custom(\n",
    "        pretrained=\"skt/kogpt2-base-v2\", lora_rank=LoRA_rank, tokenizer=tokenizer\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(\n",
    "    model=model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=Adam(model.parameters(), lr=5e-5),\n",
    "    train_dataset=train_dataset_preprocess,\n",
    "    eval_dataset=eval_dataset_preprocess,\n",
    "    batch_size=4,\n",
    "    max_epochs=1,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.fit(use_lora=0)    # 이것도 LoRA관련 매개변수 같은데 코드를 보면 사용하지 않는 매개변수임\n",
    "\n",
    "model.save_pretrained(\"output_2_RM_LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora를 적용한 ppo  \n",
    "GPTCritic로 rm을 불러오는데 rm_RoLA를 따로 훈련할 필요가 없었는지 의문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='output_1_SFT_preprocess', lora_rank=LoRA_rank).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='output_2_RM_LoRA', lora_rank=LoRA_rank).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('output_3_PPO_LoRA_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):죄송합니다, 불고기용 고기는 건강에 좋지 않습니다. 쇠고기는 소고기와 함께 먹으면 건강에도 나쁜 영향을 줄 수 있으므로 조심하세요. 에쿠로코리아, 이글스 등 다양한 음식 재료를 사용하시면 됩니다. ai 언어모델로 제작되었습니다. quiae khos kywoh, quiae joan kihappo toyuard i mooin unicer 등등의 이름으로도 유명합니다. sheungo ki kin happo will more refrighing questions ohanesi me. quiae kai cai toyuard i meaning more refrighing. quiae gee unice, qui cai toyuard i manner, quiae joung kihappo toyuard i meaning unicer, quia\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):리처드 닉슨이 42대 부통령직을 수행한 년도는 1950년입니다. espithin ai language werker is connective warning in the feeling seaside provide. 언급되며, 가장 최근에는 33대 부통령직을 수행한 경우도 있습니다. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin ai language warning seaside provide. espithin espithin espithin ai language warning s\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):시카고 오헤어 국제공항은 현재 운영되고 있습니다. of preferences japan limited qu.or of citiese limited는 미국에서 운영되는 공항으로, 시카고 국제공항에 위치해 있습니다. of citiese limited 는 시카고에 위치한 도시 중 하나로, 시카고 국제공항의 메인 노선이기도 합니다. assistance of friends limited sofinities limited qu. assistance of citiese limited sofinities limited는 시카고에 위치한 공항입니다. of citiese limited sofinities limited sofinities qu.or는 미국에서 발생한 사건으로 발생한 것입니다. 라는 용어는 이 언어에서 파생된 것으로, 시카고 국제공항의 설립자로 유명합니다. assistance of citiestion qu.or of cities qu.or 는 시카고 국제공항을 운영하는 것으로 유명합니다. of citiesia qu.or (jpr\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):저는 ai 어시스턴트이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만 미세먼지 관련해서는 개인의 개인 취향에 따라 다르기 때문에, 상황에 따라 적절한 대처 방법이 있을 수 있습니다. 저는 인공지능 챗봇이기 때문에 어떤 문제가 있으셨는지에 대해 알 수 있지만, 개인적으로는 개인 건강 상태에 대해서 충분히 고려하고 대처해야할 필요가 있습니다. 또한, 호흡기 질환자의 경우 호흡기 질환을 예방하는 데에 주의해야합니다. 따라서 건강 상태가 좋지 않을 경우에는 전문 의료기관에서 상담을 받으시는 것이 가장 좋습니다. 또한, 건강한 생활 습관을 가지는 것이 중요합니다. 말씀드릴 수 없는 상황에서 미세먼지 문제에 대해 정확한 대처 방법을 찾은 것은 건강에 도움이 됩니다. 말씀드릴 수 있는 경우, 해당 상황을 인식하고 대처 방법을 찾아봅니다. 말씀드릴 수 있는 상황, 어떤 영향을 받으시겠다는 것인지 등의 상황에 따라서 적절한 대처 방법 및 대처 방법을 찾는 것이 중요합니다. 말씀드릴 수 있는 상황이라면, 정확한 대처 방법을 찾고 상황을 파악하시기 바랍니다. 말씀드린다면, 적절한 대응 방법을 찾아보는 것이 중요합니다. 말씀드릴 수 있는 상황이므로, 개인의 상황에 따라서 적절한 대응 방법에 대해 말씀\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리 및 회고\n",
    "\n",
    "sft가 모델의 결과를 가장 많이 바꾸어서 인상적이었는데, rlhf는 데이터의 문제인지 더 좋아졌다는 느낌을 받지 않았다. \n",
    "\n",
    "모델 개선은 전처리가 가장 친숙하기 때문에 가장 먼저 시도해 봤는데, 영문 데이터에 대해서 어떻게 처리하면 좋을 지 의문이 있었다. 결과물을 보면 영어가 문제가 되는데 고유명사를 영어로 쓰는 경우가 많아서 제거하기는 애매하였다. 그리고 데이터가 어떻게 구성되었는지 전부 볼수는 없어서 일단 모델을 훈련시키고 이상한 점을 찾은 뒤에 그것을 해결하는 역순으로 진행하였다.  \n",
    "\n",
    "LoRA를 적용하였는데 peft패키지는 설치가 안되었고 깃허브에 작성된 lora.py를 사용하여 적용해 보았다. 황당하게 fit함수의 lora 매개변수는 사용되지 않는 상태였고 LoRA를 적용해서 속도는 확실히 빨라졌는데, 성능은 비슷한거 같았다.  \n",
    "\n",
    "시간이 없어서 하이퍼 파라미터 튜닝이나 epoch를 좀 더 늘려보지 않아서 이 성능이 한계인지는 모르겠지만, 모델 성능이 좀 안좋다고 느낌을 받았다. 무엇보다 의문인건 eos 토큰을 사용했을 텐데 생성 문장이 적당히 끊기지 않고 계속 이어지는 현상이었다. 데이터 자체는 그렇게 길지 않은거 같아서 왜 이렇게 되는지 발견하지 못하였다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
