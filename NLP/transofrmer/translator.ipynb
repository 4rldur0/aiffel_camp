{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머 모델은 활용한 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "plt.rc(\"font\", family=\"Malgun Gothic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터\n",
    "* 데이터 : https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거한 데이터 로드\n",
    "def clean_corpus():\n",
    "    with open(\"../data/korean-english-park.train.ko\", \"r\") as f:\n",
    "        ko = f.read().splitlines()\n",
    "    with open(\"../data/korean-english-park.train.en\", \"r\") as f:\n",
    "        en = f.read().splitlines()\n",
    "\n",
    "    df = pd.DataFrame({\"ko\": ko, \"en\": en})\n",
    "    cleaned_corpus = df.drop_duplicates([\"ko\"])\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()  # 소문자화\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)  # 기호처리\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 연속 공백 처리\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 기타 문자 제거\n",
    "\n",
    "    sentence = sentence.strip()  # 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대토큰은 25745개로 그냥 20000개 사용\n",
    "# 토큰화\n",
    "def generate_tokenizer(\n",
    "    corpus, vocab_size=20000, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3\n",
    "):\n",
    "    import sentencepiece as sp\n",
    "    import os\n",
    "\n",
    "    path = f\"korean-english-park.train.{lang}.temp\"\n",
    "    tokenizer = sp.SentencePieceProcessor()\n",
    "\n",
    "    # 토큰화 학습에 필요한 파일 생성\n",
    "    with open(path, \"w\") as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + \"\\n\")\n",
    "\n",
    "    # 토큰화 모델 학습\n",
    "    sp.SentencePieceTrainer.Train(\n",
    "        f\"--input={path} --model_prefix={lang}_spm --vocab_size={vocab_size} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}\"\n",
    "    )\n",
    "    tokenizer.Load(f\"{lang}_spm.model\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 코퍼스 생성\n",
    "cleaned_corpus = clean_corpus()\n",
    "\n",
    "cleaned_corpus[\"ko\"] = cleaned_corpus[\"ko\"].apply(lambda x: preprocess_sentence(x))\n",
    "cleaned_corpus[\"en\"] = cleaned_corpus[\"en\"].apply(lambda x: preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "ko_tokenizer = generate_tokenizer(cleaned_corpus[\"ko\"])\n",
    "en_tokenizer = generate_tokenizer(cleaned_corpus[\"en\"], lang=\"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징\n",
    "cleaned_corpus.loc[:, \"ko\"] = cleaned_corpus[\"ko\"].apply(\n",
    "    lambda x: ko_tokenizer.EncodeAsIds(x)\n",
    ")\n",
    "cleaned_corpus.loc[:, \"en\"] = cleaned_corpus[\"en\"].apply(\n",
    "    lambda x: en_tokenizer.EncodeAsIds(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이 50이하만 유지\n",
    "cleaned_corpus_short = cleaned_corpus[cleaned_corpus[\"ko\"].apply(len) <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75806, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩처리\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"ko\"],\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"en\"],\n",
    "    padding=\"post\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75806, 50), (75806, 113))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어는 토큰수가 한국어 보다 많은데 이를 50까지만 사용\n",
    "# 괜찮은건지 실험할 예정\n",
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 택1\n",
    "# 영어 토큰수로 패딩\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"ko\"],\n",
    "    maxlen=dec_train.shape[1],\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "# 한국어 토큰수로 패딩\n",
    "# dec_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#     cleaned_corpus_short[\"en\"],\n",
    "#     maxlen=enc_train.shape[1],\n",
    "#     padding=\"post\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75806, 113), (75806, 113))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드 어텐션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)  # 나누기 위하여 타입 변경\n",
    "\n",
    "        scaled_qk = tf.matmul(Q, K, transpose_b=True)  # q, k의 유사도 측정\n",
    "        scaled_qk /= tf.sqrt(d_k)  # 스케일 조정\n",
    "\n",
    "        # 마스크에 해당하는 아주 큰 값을 빼서 softmax를 통과하면 0에 수렴하게 만듦\n",
    "        if mask is not None:\n",
    "            scaled_qk += mask * -1e9\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, -1)\n",
    "        out = tf.matmul(attentions, V)  # 어텐션 가중치 반영\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x heads x length x self.depth ]\n",
    "        \"\"\"\n",
    "\n",
    "        split_x = tf.reshape(x, (x.shape[0], -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, [0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "\n",
    "        x: [ batch x heads x length x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        # split_heads의 역순 진행\n",
    "        combined_x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(\n",
    "            combined_x, (combined_x.shape[0], combined_x.shape[1], self.d_model)\n",
    "        )\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # 리니어 적용\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        # 머리 쪼개기\n",
    "        WQ_split = self.split_heads(WQ)\n",
    "        WK_split = self.split_heads(WK)\n",
    "        WV_split = self.split_heads(WV)\n",
    "\n",
    "        # 어텐션\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_split, WK_split, WV_split, mask\n",
    "        )\n",
    "\n",
    "        # 머리 합치기\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    # 실수가 아닌 것을 전부 0으로\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스킹 확인\n",
    "# batch, length = 16, 20\n",
    "# src_padding = 5\n",
    "# tgt_padding = 15\n",
    "\n",
    "# src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "# tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "# sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "# sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "# sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "# enc_mask, dec_enc_mask, dec_mask = generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# ax1 = fig.add_subplot(131)\n",
    "# ax2 = fig.add_subplot(132)\n",
    "# ax3 = fig.add_subplot(133)\n",
    "\n",
    "# ax1.set_title(\"1) Encoder Mask\")\n",
    "# ax2.set_title(\"2) Encoder-Decoder Mask\")\n",
    "# ax3.set_title(\"3) Decoder Mask\")\n",
    "\n",
    "# ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap=\"Dark2\")\n",
    "# ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap=\"Dark2\")\n",
    "# ax3.imshow(dec_mask[0, 0].numpy(), cmap=\"Dark2\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더층\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Networn\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더층\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, mask, casual_mask):\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # encoder-decoder Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(x)\n",
    "        out, enc_dec_attn = self.enc_dec_attn(out, enc_out, enc_out, casual_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 인코더\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 디코더\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](\n",
    "                out, enc_out, causality_mask, padding_mask\n",
    "            )\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        d_ff,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        pos_len,\n",
    "        dropout=0.2,\n",
    "        shared=True,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positionl = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "        if self.shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_embedding.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared:\n",
    "            out *= tf.sqrt(self.d_model)\n",
    "\n",
    "        # 포지셔널 인코딩은 batch차원이 없어서 확장 : [tf.newaxis, ...]\n",
    "        # 데이터 길이 만큼만 더하기 : [:, :x.shape[1], :]\n",
    "        pos_encoding = tf.constant(self.positionl, dtype=tf.float32)\n",
    "        out += pos_encoding[tf.newaxis, ...][:, : x.shape[1], :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # 임베딩\n",
    "        enc_in = self.embedding(self.enc_embedding, enc_in)\n",
    "        dec_in = self.embedding(self.dec_embedding, dec_in)\n",
    "\n",
    "        # 인코더\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        # 디코더\n",
    "        dec_out, dec_attns, enc_dec_attns = self.decoder(\n",
    "            dec_in, enc_out, dec_mask, causality_mask\n",
    "        )\n",
    "        # 최종 출력\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, enc_dec_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "\n",
    "        seaborn.heatmap(\n",
    "            data,\n",
    "            square=True,\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "            xticklabels=x,\n",
    "            yticklabels=y,\n",
    "        )\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, : len(src), : len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, : len(tgt), : len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer + 1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, : len(tgt), : len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [tokens], maxlen=enc_train.shape[-1], padding=\"post\"\n",
    "    )\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
    "            _input, output\n",
    "        )\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            _input, output, enc_padding_mask, combined_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predicted_id = (\n",
    "            tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        )\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(\n",
    "        sentence, model, src_tokenizer, tgt_tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Input: %s\" % (sentence))\n",
    "    print(\"Predicted translation: {}\".format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률, 옵티마이저\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step**-0.5\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return (self.d_model**-0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            src, tgt, enc_mask, dec_enc_mask, dec_mask\n",
    "        )\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다.\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1185/1185 [17:36<00:00,  1.12it/s, Loss 6.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama barack obama barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the population are expected to be .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it is no doubt .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the city of the city of the city of the country .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 1185/1185 [17:18<00:00,  1.14it/s, Loss 4.5749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is now , .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is in the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: we are not going to because they are not too far .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: earlier sunday , eight people died in the death toll .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 1185/1185 [17:15<00:00,  1.14it/s, Loss 4.0436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the city s city s city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the ban is not authorized .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 1185/1185 [17:12<00:00,  1.15it/s, Loss 3.6662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the first time .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the first to be in the cities of the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there is no alternative to the world .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: on wednesday , the second death toll from the dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 1185/1185 [17:11<00:00,  1.15it/s, Loss 3.0812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the latest person .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the town is surrounded by city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drinking .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were among the dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 1185/1185 [17:11<00:00,  1.15it/s, Loss 2.4194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second presidential nominee .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city is the urban city s urban city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there are no excuses to avoid providing coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven deaths were in the seven days .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 1185/1185 [17:11<00:00,  1.15it/s, Loss 1.7982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city streets are small , especially in the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: we need to avoid it .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people have been confirmed dead , all but seven occurred .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 1185/1185 [17:12<00:00,  1.15it/s, Loss 1.2964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president elect barack obama is a top priority .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city lives .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: doesn t get a coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven of the dead were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 1185/1185 [17:13<00:00,  1.15it/s, Loss 0.9350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the latest in a country he has .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people rent the city s lives .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: does drinking coffee is no protection .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven seven have seven babies , including seven babies and seven injured , have died since saturday , when seven have died .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1185/1185 [17:13<00:00,  1.15it/s, Loss 0.7006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is in his country .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city animals are hoern soaring .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it needs .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven of the dead were civilians .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1185/1185 [17:12<00:00,  1.15it/s, Loss 0.5609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he s a president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city animals watch this city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: he needs to sell .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven of the dead were wounded .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1185/1185 [17:13<00:00,  1.15it/s, Loss 0.4651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president elect obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citys are young people .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the no .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven other people were killed , .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1185/1185 [17:13<00:00,  1.15it/s, Loss 0.3981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has picked up a country from his democratic presidential race .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city . city food is surrounded by city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the coffee needs no crime .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people remained in custody , seven families and seven died .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:  53%|█████▎    | 626/1185 [09:06<08:06,  1.15it/s, Loss 0.2883]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 모델 설정\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "src_vocab_size = ko_tokenizer.vocab_size()\n",
    "tgt_vocab_size = en_tokenizer.vocab_size()\n",
    "pos_len = enc_train.shape[1]\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len\n",
    ")\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "example_sentence = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\",\n",
    "]\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)  # tqdm\n",
    "\n",
    "    for batch, idx in enumerate(t):\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(\n",
    "            enc_train[idx : idx + BATCH_SIZE],\n",
    "            dec_train[idx : idx + BATCH_SIZE],\n",
    "            transformer,\n",
    "            optimizer,\n",
    "        )\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        t.set_description_str(\"Epoch %2d\" % (epoch + 1))  # tqdm\n",
    "        t.set_postfix_str(\"Loss %.4f\" % (total_loss.numpy() / (batch + 1)))  # tqdm\n",
    "\n",
    "    for s in example_sentence:\n",
    "        translate(s, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험\n",
    "*epoch 별 번역 기록을 txt로 저장하여 보관*\n",
    "\n",
    "#### 1. 하이퍼 파라미터 조정 이전에, 토큰화 패딩을 무엇을 기준으로 할지를 먼저 비교. \n",
    "\n",
    " lms에서 한국어 토큰 수 50개 이하를 사용하는데, 영어의 토큰수는 이보다 많았고 어디에 맞추는 것이 좋을 지 비교.  \n",
    "  일단 실험 전에는 긴 쪽으로, 영어에 맞추는게 성능이 좋을 것이라고 생각됨. 한국어와 영어가 어순이 동일하지도 않고, 영어를 중간까지 짜른것만으로는 번역의 학습이 잘 안 될것으로 예상  \n",
    "  \n",
    "  ```python\n",
    "  파라미터 : \n",
    "        n_layers = 2\n",
    "        d_model = 256\n",
    "        n_heads = 8\n",
    "        d_ff = 256\n",
    "  ```\n",
    "  \n",
    "  ##### 1.1 한국어에 맞추었을 때  \n",
    "  sequence length : 50   \n",
    "  epoch 당 시간 : 로컬 40초대, 클라우드 1분 30초대\n",
    "  ```text\n",
    "  poch 20: 100%|██████████| 1181/1181 [00:42<00:00, 27.56it/s, Loss 0.9116]\n",
    "    Input: 오바마는 대통령이다.\n",
    "    Predicted translation: obama is as obama .\n",
    "    Input: 시민들은 도시 속에 산다.\n",
    "    Predicted translation: the city is to get to the city in the city just living , street .\n",
    "    Input: 커피는 필요 없다.\n",
    "    Predicted translation: no one does notapal .\n",
    "    Input: 일곱 명의 사망자가 발생했다.\n",
    "    Predicted translation: seven deaths were died .\n",
    "  ```\n",
    "  ##### 1.2 영어에 맞추었을 때   \n",
    "  sequence length : 129  \n",
    "  epoch 당 시간 : 로컬 1분 30초대, 클라우드 3분 40초대 (2배)  \n",
    "\n",
    "  ```text\n",
    "  Epoch 15: 100%|██████████| 1181/1181 [01:38<00:00, 11.98it/s, Loss 1.4021]\n",
    "    Input: 오바마는 대통령이다.\n",
    "    Predicted translation: obama is the president elected .\n",
    "    Input: 시민들은 도시 속에 산다.\n",
    "    Predicted translation: the city has been sitting in place .\n",
    "    Input: 커피는 필요 없다.\n",
    "    Predicted translation: drink coffee is not needing .\n",
    "    Input: 일곱 명의 사망자가 발생했다.\n",
    "    Predicted translation: on tuesday , the seventh was dead .\n",
    "  ```\n",
    "\n",
    "  ##### 1.3 결론\n",
    "  20개의 결과를 보고 가장 좋았던 것을 기록하였는데, loss 자체는 전부 1근처로 비슷하였지만, 번역문을 보면 1.2처럼 영어로 기준을 하는게 더 좋은 결과를 보이는 듯 하다. 또한 1.1의 경우 금방 비슷한 결과만 나오게 되었다. \n",
    "  \n",
    "\n",
    "#### 2. 파라미터를 조정\n",
    "\n",
    "1.3에 따라 영어 토큰 수를 기준으로 패딩하고 파라미터를 논문처럼 사용하여 학습  \n",
    "토큰화 사전 크기는 논문에서도 2~3만개 였고, 현재 데이터로 최대 크기가 25000개 가량이기 때문에 그냥 2만개 사용\n",
    "  ```python\n",
    "  파라미터 : \n",
    "        n_layers = 6\n",
    "        d_model = 512\n",
    "        n_heads = 8\n",
    "        d_ff = 2048\n",
    "  ```\n",
    "\n",
    "시간이 없어서 epoch는 13까지밖에 못해봤는데, loss가 0.4로 눈에띄게 줄어든 것을 볼 수 있었다. 다만 위의 작은 모델도 동일시간동안 epoch를 계속 돌리면 어디까지 떨어질 지는 모르겠지만 말이다. 또한 번역의 질도 정성적으로 보았을 때 작은 모델과 비교해도 loss만큼의 차이가 없었다. loss를 빼고 봐도 사실상 번역의 질은 차이를 구분할 수 없을 것 같았다.  \n",
    " \n",
    " ```text\n",
    "Epoch 13: 100%|██████████| 1185/1185 [17:13<00:00,  1.15it/s, Loss 0.3981]\n",
    "  Input: 오바마는 대통령이다.\n",
    "  Predicted translation: obama has picked up a country from his democratic presidential race .\n",
    "  Input: 시민들은 도시 속에 산다.\n",
    "  Predicted translation: city . city food is surrounded by city .\n",
    "  Input: 커피는 필요 없다.\n",
    "  Predicted translation: the coffee needs no crime .\n",
    "  Input: 일곱 명의 사망자가 발생했다.\n",
    "  Predicted translation: seven other people remained in custody , seven families and seven died .\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론 및 회고\n",
    "\n",
    "이번 프로젝트 동안 2가지 실험을 진행하였다.   \n",
    "1. 인코더 입력과 디코더 입력의 길이를 맞추어 줄 때 무엇을 기준으로 잡는 것이 좋을지\n",
    "2. 모델의 규모를 논문 수준으로 키우면 번역의 질이 올라갈 지 \n",
    "\n",
    "다만 데이터의 부족으로 테스트데이터를 사용하지 못하여 over fitting에 대한 것은 알 수가 없었다.  \n",
    "\n",
    "1에서는 loss자체는 비슷하게 떨어지지만, 정성적인 평가나 직관으로는 긴 쪽으로 맞추는 것이 좋아보였다. 다만 길이가 그대로 연산량과 선형적으로 비례하였기 때문에 주의가 필요해 보인다. 이번 실험에서는 연산량이 늘어난 것 치고 성능이 오르지 않았다고 볼 수 있었다. \n",
    "\n",
    "2는 loss가 많이 떨어졌 기대가 되었지만, 번역의 질은 오르지 못한것으로 판단되었다. loss를 보면 학습은 잘 되는것 같은데, 사실 데이터를 살펴보면 이게 번역 쌍이 맞는지 의문이들 정도로 이상한 데이터가 자주 보여서 학습 데이터가 문제가 되었을 가능성이 있다고 생각하였다.  \n",
    "\n",
    "\n",
    "일단 논문에서 단어사전 크기가 2~3만개 정도라 이를 1만 정도로 축소 조정하는 것을 후순위에 두고 있어서 못 해본 것이 아쉽고, 많은 시간을 들여 큰 모델을 학습 시켜도 좋은 번역이 안나와서 아쉬웠다. seq2seq에서 잘 안나온 경우가 많아서 이것도 만족하는 다른 분들이 계셨지만, 운이 좋게 seq2seq가 잘 나와서 이번게 조금 실망이 되었다.  \n",
    "다만 seq2seq와 조금 다른 경향성이 보이는 듯 한게 환각현상이었다. seq2seq에서는 단어 자체를 잘못 예측하는게 주였지만, 이번 트랜스포머에서는 단어를 잘 예측을 하는데 사족이 너무나도 많았다. 예를들어 '일곱 명의 사상자가 발생했다'를 번역할 때 없던 요일 정보를 포함해서 번역하는 일이 많이 있었다.  \n",
    "이번 모델 비교를 하면서 자연어 처리에서의 고충을 느끼게 되었는데, 대체 어떤 모델이 더 좋은 모델인지 알기가 힘들었다. 단순히 loss만 봐도 되는건지, 예측한 번역문을 보는게 진짜 의미가 있는지 의문이 들었고, 규모가 다른 모델을 비교할 때 역시 어떻게 보면 좋을지를 고민하게 되었다. 퍼실님께서는 예문 3,4개 정도만 봐도 성능 확인이 괜찮다고는 하셨지만, seq2seq에서 너무 예문만 딱 잘 번역하고 나머지는 별로였던 경우가 있었어서 더 고민이 되었다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
