{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머 모델은 활용한 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "plt.rc(\"font\", family=\"Malgun Gothic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터\n",
    "* 데이터 : https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거한 데이터 로드\n",
    "def clean_corpus():\n",
    "    with open(\"../data/korean-english-park.train.ko\", \"r\") as f:\n",
    "        ko = f.read().splitlines()\n",
    "    with open(\"../data/korean-english-park.train.en\", \"r\") as f:\n",
    "        en = f.read().splitlines()\n",
    "\n",
    "    df = pd.DataFrame({\"ko\": ko, \"en\": en})\n",
    "    cleaned_corpus = df.drop_duplicates([\"ko\"])\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()  # 소문자화\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)  # 기호처리\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 연속 공백 처리\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 기타 문자 제거\n",
    "\n",
    "    sentence = sentence.strip()  # 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "def generate_tokenizer(\n",
    "    corpus, vocab_size=20000, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3\n",
    "):\n",
    "    import sentencepiece as sp\n",
    "    import os\n",
    "\n",
    "    path = f\"model/korean-english-park.train.{lang}.temp\"\n",
    "    tokenizer = sp.SentencePieceProcessor()\n",
    "\n",
    "    # 처음 학습한 sp 모델이 있는 지 확인\n",
    "    if not os.path.exists(f\"{lang}_spm.model\"):\n",
    "        # 토큰화 학습에 필요한 파일 생성\n",
    "        with open(path, \"w\") as f:\n",
    "            for row in corpus:\n",
    "                f.write(str(row) + \"\\n\")\n",
    "\n",
    "        # 토큰화 모델 학습\n",
    "        sp.SentencePieceTrainer.Train(\n",
    "            f\"--input={path} --model_prefix={lang}_spm --vocab_size={vocab_size} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}\"\n",
    "        )\n",
    "    tokenizer.Load(f\"{lang}_spm.model\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 코퍼스 생성\n",
    "cleaned_corpus = clean_corpus()\n",
    "\n",
    "cleaned_corpus[\"ko\"] = cleaned_corpus[\"ko\"].apply(lambda x: preprocess_sentence(x))\n",
    "cleaned_corpus[\"en\"] = cleaned_corpus[\"en\"].apply(lambda x: preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "ko_tokenizer = generate_tokenizer(cleaned_corpus[\"ko\"])\n",
    "en_tokenizer = generate_tokenizer(cleaned_corpus[\"en\"], lang=\"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징\n",
    "cleaned_corpus.loc[:, \"ko\"] = cleaned_corpus[\"ko\"].apply(\n",
    "    lambda x: ko_tokenizer.EncodeAsIds(x)\n",
    ")\n",
    "cleaned_corpus.loc[:, \"en\"] = cleaned_corpus[\"en\"].apply(\n",
    "    lambda x: en_tokenizer.EncodeAsIds(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이 50이하만 유지\n",
    "cleaned_corpus_short = cleaned_corpus[cleaned_corpus[\"ko\"].apply(len) <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ko</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1071, 358, 522, 480, 7, 1285, 1879, 8, 1204, ...</td>\n",
       "      <td>[1, 354, 11, 1466, 8806, 14, 23, 59, 115, 128,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[172, 3249, 390, 3710, 27, 3475, 186, 19, 3249...</td>\n",
       "      <td>[1, 186, 10, 2899, 10, 431, 564, 348, 59, 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[69, 1204, 8, 144, 11443, 28, 5495, 438, 463, 4]</td>\n",
       "      <td>[1, 144, 102, 13430, 190, 2925, 6, 56, 33, 77,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 139, 42, 19, 5739, 3652, 3249, 390, 3710, ...</td>\n",
       "      <td>[1, 323, 7, 1697, 16, 91, 14046, 302, 6652, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[19, 465, 189, 12, 823, 71, 12, 796, 1135, 16,...</td>\n",
       "      <td>[1, 1804, 936, 6, 33, 115, 27, 154, 8, 12, 150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94118</th>\n",
       "      <td>[103, 6, 46, 20, 9, 19582, 762, 25, 22, 7, 501...</td>\n",
       "      <td>[1, 104, 40, 1768, 14, 9, 6096, 1371, 864, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94119</th>\n",
       "      <td>[1562, 5419, 12937, 61, 8, 19009, 25, 12511, 2...</td>\n",
       "      <td>[1, 20, 5230, 2885, 6, 101, 4344, 8, 195, 7, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94120</th>\n",
       "      <td>[69, 15632, 17, 1019, 1235, 18, 6496, 1831, 14...</td>\n",
       "      <td>[1, 56, 4, 1440, 11, 1371, 864, 23, 539, 14, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94121</th>\n",
       "      <td>[19009, 25, 22, 7, 896, 8, 4053, 3927, 1283, 4...</td>\n",
       "      <td>[1, 18, 6256, 13, 39, 975, 372, 3573, 4, 1385,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94122</th>\n",
       "      <td>[94, 8, 19009, 7, 7681, 16, 398, 1019, 10, 9, ...</td>\n",
       "      <td>[1, 101, 40, 554, 14, 9, 789, 290, 2485, 7, 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75579 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ko  \\\n",
       "0      [1071, 358, 522, 480, 7, 1285, 1879, 8, 1204, ...   \n",
       "1      [172, 3249, 390, 3710, 27, 3475, 186, 19, 3249...   \n",
       "2       [69, 1204, 8, 144, 11443, 28, 5495, 438, 463, 4]   \n",
       "3      [4, 139, 42, 19, 5739, 3652, 3249, 390, 3710, ...   \n",
       "7      [19, 465, 189, 12, 823, 71, 12, 796, 1135, 16,...   \n",
       "...                                                  ...   \n",
       "94118  [103, 6, 46, 20, 9, 19582, 762, 25, 22, 7, 501...   \n",
       "94119  [1562, 5419, 12937, 61, 8, 19009, 25, 12511, 2...   \n",
       "94120  [69, 15632, 17, 1019, 1235, 18, 6496, 1831, 14...   \n",
       "94121  [19009, 25, 22, 7, 896, 8, 4053, 3927, 1283, 4...   \n",
       "94122  [94, 8, 19009, 7, 7681, 16, 398, 1019, 10, 9, ...   \n",
       "\n",
       "                                                      en  \n",
       "0      [1, 354, 11, 1466, 8806, 14, 23, 59, 115, 128,...  \n",
       "1      [1, 186, 10, 2899, 10, 431, 564, 348, 59, 10, ...  \n",
       "2      [1, 144, 102, 13430, 190, 2925, 6, 56, 33, 77,...  \n",
       "3      [1, 323, 7, 1697, 16, 91, 14046, 302, 6652, 7,...  \n",
       "7      [1, 1804, 936, 6, 33, 115, 27, 154, 8, 12, 150...  \n",
       "...                                                  ...  \n",
       "94118  [1, 104, 40, 1768, 14, 9, 6096, 1371, 864, 9, ...  \n",
       "94119  [1, 20, 5230, 2885, 6, 101, 4344, 8, 195, 7, 3...  \n",
       "94120  [1, 56, 4, 1440, 11, 1371, 864, 23, 539, 14, 3...  \n",
       "94121  [1, 18, 6256, 13, 39, 975, 372, 3573, 4, 1385,...  \n",
       "94122  [1, 101, 40, 554, 14, 9, 789, 290, 2485, 7, 21...  \n",
       "\n",
       "[75579 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩처리\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"ko\"],\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"en\"],\n",
    "    padding=\"post\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75579, 50), (75579, 129))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어는 토큰수가 한국어 보다 많은데 이를 50까지만 사용\n",
    "# 괜찮은건지 실험할 예정\n",
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cleaned_corpus_short[\"ko\"],\n",
    "    maxlen=dec_train.shape[1],\n",
    "    padding=\"post\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75579, 129), (75579, 129))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드 어텐션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)  # 나누기 위하여 타입 변경\n",
    "\n",
    "        scaled_qk = tf.matmul(Q, K, transpose_b=True)  # q, k의 유사도 측정\n",
    "        scaled_qk /= tf.sqrt(d_k)  # 스케일 조정\n",
    "\n",
    "        # 마스크에 해당하는 아주 큰 값을 빼서 softmax를 통과하면 0에 수렴하게 만듦\n",
    "        if mask is not None:\n",
    "            scaled_qk += mask * -1e9\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, -1)\n",
    "        out = tf.matmul(attentions, V)  # 어텐션 가중치 반영\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x heads x length x self.depth ]\n",
    "        \"\"\"\n",
    "\n",
    "        split_x = tf.reshape(x, (x.shape[0], -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, [0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "\n",
    "        x: [ batch x heads x length x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        # split_heads의 역순 진행\n",
    "        combined_x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(\n",
    "            combined_x, (combined_x.shape[0], combined_x.shape[1], self.d_model)\n",
    "        )\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # 리니어 적용\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        # 머리 쪼개기\n",
    "        WQ_split = self.split_heads(WQ)\n",
    "        WK_split = self.split_heads(WK)\n",
    "        WV_split = self.split_heads(WV)\n",
    "\n",
    "        # 어텐션\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_split, WK_split, WV_split, mask\n",
    "        )\n",
    "\n",
    "        # 머리 합치기\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    # 실수가 아닌 것을 전부 0으로\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스킹 확인\n",
    "# batch, length = 16, 20\n",
    "# src_padding = 5\n",
    "# tgt_padding = 15\n",
    "\n",
    "# src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "# tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "# sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "# sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "# sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "# enc_mask, dec_enc_mask, dec_mask = generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# ax1 = fig.add_subplot(131)\n",
    "# ax2 = fig.add_subplot(132)\n",
    "# ax3 = fig.add_subplot(133)\n",
    "\n",
    "# ax1.set_title(\"1) Encoder Mask\")\n",
    "# ax2.set_title(\"2) Encoder-Decoder Mask\")\n",
    "# ax3.set_title(\"3) Decoder Mask\")\n",
    "\n",
    "# ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap=\"Dark2\")\n",
    "# ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap=\"Dark2\")\n",
    "# ax3.imshow(dec_mask[0, 0].numpy(), cmap=\"Dark2\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더층\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Networn\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더층\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, mask, casual_mask):\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # encoder-decoder Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(x)\n",
    "        out, enc_dec_attn = self.enc_dec_attn(out, enc_out, enc_out, casual_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 인코더\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 디코더\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](\n",
    "                out, enc_out, causality_mask, padding_mask\n",
    "            )\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        d_ff,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        pos_len,\n",
    "        dropout=0.2,\n",
    "        shared=True,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positionl = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "        if self.shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_embedding.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared:\n",
    "            out *= tf.sqrt(self.d_model)\n",
    "\n",
    "        # 포지셔널 인코딩은 batch차원이 없어서 확장 : [tf.newaxis, ...]\n",
    "        # 데이터 길이 만큼만 더하기 : [:, :x.shape[1], :]\n",
    "        pos_encoding = tf.constant(self.positionl, dtype=tf.float32)\n",
    "        out += pos_encoding[tf.newaxis, ...][:, : x.shape[1], :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # 임베딩\n",
    "        enc_in = self.embedding(self.enc_embedding, enc_in)\n",
    "        dec_in = self.embedding(self.dec_embedding, dec_in)\n",
    "\n",
    "        # 인코더\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        # 디코더\n",
    "        dec_out, dec_attns, enc_dec_attns = self.decoder(\n",
    "            dec_in, enc_out, dec_mask, causality_mask\n",
    "        )\n",
    "        # 최종 출력\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, enc_dec_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "\n",
    "        seaborn.heatmap(\n",
    "            data,\n",
    "            square=True,\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "            xticklabels=x,\n",
    "            yticklabels=y,\n",
    "        )\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, : len(src), : len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, : len(tgt), : len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer + 1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, : len(tgt), : len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [tokens], maxlen=enc_train.shape[-1], padding=\"post\"\n",
    "    )\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
    "            _input, output\n",
    "        )\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            _input, output, enc_padding_mask, combined_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predicted_id = (\n",
    "            tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        )\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(\n",
    "        sentence, model, src_tokenizer, tgt_tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Input: %s\" % (sentence))\n",
    "    print(\"Predicted translation: {}\".format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률, 옵티마이저\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step**-0.5\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return (self.d_model**-0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            src, tgt, enc_mask, dec_enc_mask, dec_mask\n",
    "        )\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다.\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1181/1181 [01:41<00:00, 11.59it/s, Loss 6.1552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s obama s obama s obama s obama s obama s obama s obama s obama se .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the first time , the first time .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the dow is not .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the wounded were killed to the attack .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 1181/1181 [01:35<00:00, 12.31it/s, Loss 4.6437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been a campaign .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the biggest is across the world .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i think you can do not .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was reported .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 1181/1181 [01:36<00:00, 12.26it/s, Loss 4.1201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city of the city s city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the situation is not immediately .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 1181/1181 [01:37<00:00, 12.15it/s, Loss 3.7381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president of the president s president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s cities city of cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee coffee is not a problem .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the dead were killed , the dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 1181/1181 [01:36<00:00, 12.30it/s, Loss 3.3515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president of the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: residents in the city of cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: those who don t need anyway .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 1181/1181 [01:37<00:00, 12.09it/s, Loss 3.0309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s obama is a way .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the city s town of cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: alternative alternative alternative alternatives .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: four people were killed in the death toll .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 1181/1181 [01:36<00:00, 12.21it/s, Loss 2.7546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a state of president bush .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is cities in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: needte needs to be necessary .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: four deaths were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 1181/1181 [01:37<00:00, 12.09it/s, Loss 2.5132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is in the obama campaign .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: residents are urban urban urban .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: need to need to be necessary .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two other people were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 1181/1181 [01:38<00:00, 12.02it/s, Loss 2.2956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is urban town .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: need to be needed .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death of seven deaths were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1181/1181 [01:37<00:00, 12.13it/s, Loss 2.1008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a great man .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the cities is cities , and cities in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t need to be bad choice .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seventeen people were killed in the attack on the oscarhurs , which houses said .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1181/1181 [01:37<00:00, 12.09it/s, Loss 1.9263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the rally came as the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee is not needed .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven deaths were deaths .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1181/1181 [01:36<00:00, 12.22it/s, Loss 1.7717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s president elect obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city has placed in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee is not need to be needd .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven others were dead in the deaths saturday evening .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1181/1181 [01:36<00:00, 12.21it/s, Loss 1.6337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s president elect barack obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the cities now , cities closed .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee is not need for change .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people dead , were wounded .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1181/1181 [01:37<00:00, 12.12it/s, Loss 1.5135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president elected president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protest is slittle in cities .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t need to be speechdo speechverver\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people were dead in the death toll tuesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1181/1181 [01:38<00:00, 11.98it/s, Loss 1.4021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president elected .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city has been sitting in place .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee is not needing .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: on tuesday , the seventh was dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1181/1181 [01:37<00:00, 12.08it/s, Loss 1.3010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president elect to president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protest cities now came in .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee change was not immediately clear .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven deaths were deaths tuesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1181/1181 [01:37<00:00, 12.16it/s, Loss 1.2081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president elected president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they re close to the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee change is not need you need to do .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the nine year old was dead sunday evening in the deaths saturday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1181/1181 [01:39<00:00, 11.89it/s, Loss 1.1243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama would oppose it .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protest is sitting in the mountain .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee is not prepared to speech .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six other people were dead tuesday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1181/1181 [01:37<00:00, 12.13it/s, Loss 1.0482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the most important thing for him .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: protesting cities s calm in .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: drink coffee is not prepared to speak in .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: tuesday s death toll could , which has been deaths in several days .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1181/1181 [01:38<00:00, 12.04it/s, Loss 0.9778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president elect he s going to be president elect barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protest in cities is staying .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t need to speech don t care .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the nine year old eleven death saturday was dead .\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 모델 설정\n",
    "n_layers = 2\n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "d_ff = 256\n",
    "src_vocab_size = ko_tokenizer.vocab_size()\n",
    "tgt_vocab_size = en_tokenizer.vocab_size()\n",
    "pos_len = enc_train.shape[1]\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len\n",
    ")\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "example_sentence = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\",\n",
    "]\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)  # tqdm\n",
    "\n",
    "    for batch, idx in enumerate(t):\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(\n",
    "            enc_train[idx : idx + BATCH_SIZE],\n",
    "            dec_train[idx : idx + BATCH_SIZE],\n",
    "            transformer,\n",
    "            optimizer,\n",
    "        )\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        t.set_description_str(\"Epoch %2d\" % (epoch + 1))  # tqdm\n",
    "        t.set_postfix_str(\"Loss %.4f\" % (total_loss.numpy() / (batch + 1)))  # tqdm\n",
    "\n",
    "    for s in example_sentence:\n",
    "        translate(s, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 결과 비교\n",
    "*epoch 별 번역 기록을 txt로 저장하여 보관*\n",
    "\n",
    "#### 1. 하이퍼 파라미터 조정 이전에, 토큰화 패딩을 무엇을 기준으로 할지를 먼저 비교. \n",
    "\n",
    " lms에서 한국어 토큰 수 50개 이하를 사용하는데, 영어의 토큰수는 이보다 많았고 어디에 맞추는 것이 좋을 지 비교.  \n",
    "  일단 실험 전에는 긴 쪽으로, 영어에 맞추는게 성능이 좋을 것이라고 생각됨. 한국어와 영어가 어순이 동일하지도 않고, 영어를 중간까지 짜른것만으로는 번역의 학습이 잘 안 될것으로 예상  \n",
    "  \n",
    "  ##### 1.1 한국어에 맞추었을 때  \n",
    "  sequence length : 50   \n",
    "  epoch 당 시간 : 40초대\n",
    "  ```text\n",
    "  poch 20: 100%|██████████| 1181/1181 [00:42<00:00, 27.56it/s, Loss 0.9116]\n",
    "    Input: 오바마는 대통령이다.\n",
    "    Predicted translation: obama is as obama .\n",
    "    Input: 시민들은 도시 속에 산다.\n",
    "    Predicted translation: the city is to get to the city in the city just living , street .\n",
    "    Input: 커피는 필요 없다.\n",
    "    Predicted translation: no one does notapal .\n",
    "    Input: 일곱 명의 사망자가 발생했다.\n",
    "    Predicted translation: seven deaths were died .\n",
    "  ```\n",
    "  ##### 1.2 영어에 맞추었을 때   \n",
    "  sequence length : 129  \n",
    "  epoch 당 시간 : 1분 30초대(2배)  \n",
    "\n",
    "  ```text\n",
    "  Epoch 15: 100%|██████████| 1181/1181 [01:38<00:00, 11.98it/s, Loss 1.4021]\n",
    "    Input: 오바마는 대통령이다.\n",
    "    Predicted translation: obama is the president elected .\n",
    "    Input: 시민들은 도시 속에 산다.\n",
    "    Predicted translation: the city has been sitting in place .\n",
    "    Input: 커피는 필요 없다.\n",
    "    Predicted translation: drink coffee is not needing .\n",
    "    Input: 일곱 명의 사망자가 발생했다.\n",
    "    Predicted translation: on tuesday , the seventh was dead .\n",
    "  ```\n",
    "\n",
    "  ##### 1.3 결론\n",
    "  20개의 결과를 보고 가장 좋았던 것을 기록하였는데, 1.2처럼 영어로 기준을 하는게 더 좋은 결과를 보이는 듯 하다. 또한 1.1의 경우 금방 비슷한 결과만 나오게 되었다. \n",
    "  \n",
    "\n",
    "#### 2. 파라미터를 조정\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
