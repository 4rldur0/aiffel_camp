{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257dad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b6fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cff20a",
   "metadata": {},
   "source": [
    "### 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad06e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/custom_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0527a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "\n",
    "data['label'] = data['class'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897b93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = sentence.replace(\"\\n\", \"\")         # 구분자\n",
    "    sentence = sentence.replace(\"\\r\", \"\")         # 구분자\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z,가-힣,0-9, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9\\.\\?\\!,]\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e51896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    half_index = len(text) // 2\n",
    "    return text[:half_index], text[half_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "093393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'idx': [],\n",
    "    'class': [],\n",
    "    'conversation': [],\n",
    "    'label': []\n",
    "}\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    part1, part2 = split_text(row['conversation'])\n",
    "    new_data['idx'].append(row['idx'])\n",
    "    new_data['class'].append(row['class'])\n",
    "    new_data['conversation'].append(part1)\n",
    "    new_data['label'].append(row['label'])\n",
    "    \n",
    "    new_data['idx'].append(row['idx'])\n",
    "    new_data['class'].append(row['class'])\n",
    "    new_data['conversation'].append(part2)\n",
    "    new_data['label'].append(row['label'])\n",
    "\n",
    "# 새로운 데이터 프레임 생성\n",
    "new_df = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc533cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['conversation'] = new_df['conversation'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2deeb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepcocessed = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f82b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  119.16059405940594\n",
      "문장길이 최대 :  453\n",
      "문장길이 표준편차 :  49.31918422196743\n",
      "pad_sequences maxlen :  100\n",
      "전체 문장의 35.71287128712871%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# max len길이를 보기위해\n",
    "total_data_text = list(new_df['conversation'])\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens  = np.mean(num_tokens) + 1 * np.std(num_tokens)\n",
    "max_tokens = 100\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)*100}%가 maxlen 설정값 이내에 포함됩니다. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8bbd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "\n",
    "MAX_WORDS = 5000\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79bebea",
   "metadata": {},
   "source": [
    "### 서브워드 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986908ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer_sw = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(new_df['conversation'] , target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ab8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer_sw.vocab_size], [tokenizer_sw.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer_sw.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa1a1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs):\n",
    "    outputs = []\n",
    "  \n",
    "    for sentence in inputs:\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 28 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence) <= MAX_LENGTH :\n",
    "            outputs.append(sentence)    \n",
    "        \n",
    "    # 최대 길이 28으로 모든 데이터셋을 패딩\n",
    "    outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sw = tokenize_and_filter(new_df['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1574fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(new_df['label']) # label로 주어 올바른 순서로 훈련\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_sw, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(np.unique(y_train)))\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b77746",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df74235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "EPOCH_FT = 100\n",
    "BUCKET_FT = 20000\n",
    "NGRAM_FT = 2\n",
    "DIM_FT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64415f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(x):\n",
    "    label = x['label']\n",
    "    return f'''__label__{label}    {x['conversation']}'''\n",
    "    \n",
    "\n",
    "add_label_text = data_prepcocessed.apply(add_label, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1548cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'input_fasttext_conversation.txt'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(add_label_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0b5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad995475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  72778\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread: 1073848 lr:  0.000000 avg.loss:  0.027765 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model_ft = fasttext.train_supervised(input=file_path,\n",
    "                                  epoch=EPOCH_FT,\n",
    "                                  bucket = BUCKET_FT,\n",
    "                                  lr = 1\n",
    "                                  wordNgrams=NGRAM_FT,\n",
    "                                  dim=DIM_FT,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bfda8",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6210ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 128)          640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 747,397\n",
      "Trainable params: 747,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(MAX_WORDS, 128, input_length=MAX_LEN))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dense(len(np.unique(data['class'])), activation='softmax'))\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4edbe3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 44s 755ms/step - loss: 1.4234 - accuracy: 0.3518 - val_loss: 1.1272 - val_accuracy: 0.4851\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_LSTM = 1\n",
    "BATCH_SIZE_LSTM = 64\n",
    "\n",
    "history = model_lstm.fit(X_train, y_train, \n",
    "                    epochs=EPOCHS_LSTM, \n",
    "                    batch_size=BATCH_SIZE_LSTM, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fde2b",
   "metadata": {},
   "source": [
    "### tranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48a00679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ecdc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "NUM_LAYERS = 12 # 인코더와 디코더의 층의 개수 \n",
    "D_MODEL = 128 # 인코더와 디코더 내부의 입/출력의 고정 차원\n",
    "NUM_HEADS = 4 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 256 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "NUM_CLASSES = len(data['class'].unique())  #레이블 수\n",
    "VOCAB_SIZE = MAX_WORDS #단어사전 크기\n",
    "MAX_LENGTH = X_train.shape[1] # maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f77daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= transformer.transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6180755c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 23s 149ms/step - loss: 1.7568 - accuracy: 0.2010 - val_loss: 1.6398 - val_accuracy: 0.1911\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbdcbc",
   "metadata": {},
   "source": [
    "## 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d3547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/custom_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa2668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['conversation'] = test['conversation'].apply(preprocess_sentence)\n",
    "tokenizer.fit_on_texts(test['conversation'].values)\n",
    "\n",
    "test_input = tokenizer.texts_to_sequences(test['conversation'].values)\n",
    "test_input = pad_sequences(test_input, maxlen=MAX_LEN_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ffedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ft =  model_ft.predict(list(test['conversation']), k=-1)\n",
    "pred_lstm = model_lstm.predict(test_input)\n",
    "pred_transformer = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f83b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred_ft[1] + pred_lstm + pred_transformer\n",
    "\n",
    "predicted_classes = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3360358",
   "metadata": {},
   "source": [
    "## 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e988c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"data/new_submission.csv\")\n",
    "sub['class']=predicted_classes\n",
    "# sub.to_csv('data/sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f691bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>t_495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>t_496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>t_497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>t_498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>t_499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name  class\n",
       "0       t_000      0\n",
       "1       t_001      0\n",
       "2       t_002      0\n",
       "3       t_003      4\n",
       "4       t_004      0\n",
       "..        ...    ...\n",
       "495     t_495      0\n",
       "496     t_496      0\n",
       "497     t_497      0\n",
       "498     t_498      0\n",
       "499     t_499      0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4486889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
